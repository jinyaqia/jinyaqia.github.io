[{"title":"flink作业提交源码解析(2) - StreamGraph的生成","url":"/flink/stream-graph-jar/","content":"\n# flink作业提交源码解析(2) - StreamGraph的生成 \n\n## WordCount源码及执行流程概览\n\n上文[flink作业提交源码解析(1)](flink-client.md)中说道`fink-client`中的方法`callMainMethod`使用了反射机制，去运行用户代码的入口类。本文就进一步研究用户的代码执行的逻辑。\n\n使用自带的WordCount.jar作为例子。运行命令如下：\n\n```java\nbin/flink run -t remote -d ./examples/streaming/WordCount.jar\n```\n<!--more-->\nwordCount的代码如下：\n\n```java\n public static void main(String[] args) throws Exception {\n\n        // Checking input parameters\n        final MultipleParameterTool params = MultipleParameterTool.fromArgs(args);\n\n        // set up the execution environment\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        // make parameters available in the web interface\n        env.getConfig().setGlobalJobParameters(params);\n\n        // get input data\n        DataStream<String> text = null;\n        if (params.has(\"input\")) {\n            // union all the inputs from text files\n            for (String input : params.getMultiParameterRequired(\"input\")) {\n                if (text == null) {\n                    text = env.readTextFile(input);\n                } else {\n                    text = text.union(env.readTextFile(input));\n                }\n            }\n            Preconditions.checkNotNull(text, \"Input DataStream should not be null.\");\n        } else {\n            System.out.println(\"Executing WordCount example with default input data set.\");\n            System.out.println(\"Use --input to specify file input.\");\n            // get default test text data\n            text = env.fromElements(WordCountData.WORDS);\n        }\n\n        DataStream<Tuple2<String, Integer>> counts =\n                // split up the lines in pairs (2-tuples) containing: (word,1)\n                text.flatMap(new Tokenizer())\n                        // group by the tuple field \"0\" and sum up tuple field \"1\"\n                        .keyBy(value -> value.f0)\n                        .sum(1);\n\n        // emit result\n        if (params.has(\"output\")) {\n            counts.writeAsText(params.get(\"output\"));\n        } else {\n            System.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n            counts.print();\n        }\n        // execute program\n        env.execute(\"Streaming WordCount\");\n    }\n```\n\n上面代码中，当执行`env.execute(\"Streaming WordCount\")`,会调用具体的`ExecutionEnvironment`去提交作业，这里是`StreamExecutionEnvironment`,如下面代码所示，逻辑便是生成streamGraph，然后执行。\n\n```java\n//StreamExecutionEnvironment \npublic JobExecutionResult execute(String jobName) throws Exception {\n        Preconditions.checkNotNull(jobName, \"Streaming Job name should not be null.\");\n        final StreamGraph streamGraph = getStreamGraph();\n        streamGraph.setJobName(jobName);\n        return execute(streamGraph);\n    }\n```\n\n## 获取和生成`StreamExecutionEnvironment`\n\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n```\n\n获取一个执行环境`ExecutionEnvironment`\n\n```java\n//StreamExecutionEnvironment\n/**\n * Creates an execution environment that represents the context in which the program is\n * currently executed. If the program is invoked standalone, this method returns a local\n * execution environment, as returned by {@link #createLocalEnvironment()}.\n *\n * @return The execution environment of the context in which the program is executed.\n */\npublic static StreamExecutionEnvironment getExecutionEnvironment() {\n    return getExecutionEnvironment(new Configuration());\n}\n```\n\n通过一个`StreamExecutionEnvironmentFactory`去获取，这里的`contextEnvironmentFactory`是`client`在执行用户代码前初始化好的。\n\n```java\n//StreamExecutionEnvironment\n\npublic static StreamExecutionEnvironment getExecutionEnvironment(Configuration configuration) {\n    return Utils.resolveFactory(threadLocalContextEnvironmentFactory, contextEnvironmentFactory)\n            .map(factory -> factory.createExecutionEnvironment(configuration))\n            .orElseGet(() -> StreamExecutionEnvironment.createLocalEnvironment(configuration));\n}\n```\n\n![image-20220123183231772](../../images/flink/stream-graph-jar/stream-1.png)\n\n当wordCount代码执行到`env.execute(\"Streaming WordCount\")`时，可以看到`text`这个`DataStream`，包含了`LegacySourceTransformation`\n\n![image-20220124095813372](../../images/flink/stream-graph-jar/stream-2.png)\n\n`counts`这个则包含一个`ReduceTransformation`\n\n![image-20220124100012457](../../images/flink/stream-graph-jar/stream-3.png)\n\n获取`StreamGraph`的逻辑在`final StreamGraph streamGraph = getStreamGraph();`这行代码的`getStreamGraph()`方法中，一步步跟进去\n\n```java\n//StreamExecutionEnvironment\n@Internal\npublic StreamGraph getStreamGraph() {\n\treturn getStreamGraph(true);\n}\n//这里执行完生成StreamGraph之后，会清空StreamExecutionEnvironment的Transformations列表\n@Internal\npublic StreamGraph getStreamGraph(boolean clearTransformations) {\n\tfinal StreamGraph streamGraph = getStreamGraphGenerator(transformations).generate();\n\tif (clearTransformations) {\n\t\ttransformations.clear();\n\t}\n\treturn streamGraph;\n}\n\nprivate StreamGraphGenerator getStreamGraphGenerator(List<Transformation<?>> transformations) {\n    if (transformations.size() <= 0) {\n        throw new IllegalStateException(\n                \"No operators defined in streaming topology. Cannot execute.\");\n    }\n    return new StreamGraphGenerator(transformations, config, checkpointCfg, configuration)\n            .setStateBackend(defaultStateBackend)\n            .setChangelogStateBackendEnabled(changelogStateBackendEnabled)\n            .setSavepointDir(defaultSavepointDirectory)\n            .setChaining(isChainingEnabled)\n            .setUserArtifacts(cacheFile)\n            .setTimeCharacteristic(timeCharacteristic)\n            .setDefaultBufferTimeout(bufferTimeout)\n            .setSlotSharingGroupResource(slotSharingGroupResources);\n}\n```\n\n最终会使用`StreamGraphGenerator`这个生成器来生成`StreamGraph`, 打印需要转换的`transformations`，该作业包含了3个`Transformation`,\n\n![image-20220124102511394](../../images/flink/stream-graph-jar/stream-4.png)\n\n## StreamGraph生成流程总览\n\n- 首先，在env中生成一颗Transformations树，存储在`List<Transformation<?>> transformatinos`中，如下图所示，包含了3个`Transformation`\n- 其次，遍历`transformatinos`\n  - `OneInputTransformation`：\n    1. 获取到`input`为`LegacySourceTransformation`，生成了`Source：Collection Source`这个`StreamNode`\n    2. 处理`OneInputTransformation`，生成`Flat Map`这个`StreamNode`\n    3. 添加`StreamEdge`（`Source: Collection Source-1_Flat Map-2_0_FORWARD_0`）连接上游`Source：Collection Source`和`Flat Map`，由于上下游并行度一致且没有指定分区方式，所以这里分区方式是`FORWARD`\n  - `RedudeTransformation`:\n    1. 获取到`input`为`PartitionTransformation`，该`Transformation`不会生成`StreamNode`,只是生成一个虚拟的分区节点，记录在`StreamGraph`对象的`virtualPartitionNodes`属性中\n    2. 处理`RedudeTransformation`，生成`Keyed Aggregation`这个`StreamNode`\n    3. 添加`streamEdge`连接上游和自身，发现上游是虚拟分区节点，从`virtualPartitionNodes`获取到上游的`StreamNode`即`Flat Map`,生成`StreamEdge`（`Flat Map-2_Keyed Aggregation-4_0_HASH_0`）连接`Flat Map`和`Keyed Aggregation`，这里指定了分区方式为`HASH`\n  - `LegacySinkTransformation`:\n    1. 获取到`input`为`ReduceTransformation`，该节点已生成\n    2. 处理`LegacySinkTransformation`，生成`Sink: Print to Std. Out`这个`StreamNode`\n    3. 添加`StreamEdge`（`Keyed Aggregation-4_Sink: Print to Std. Out-5_0_FORWARD_0`）连接上游`Keyed Aggregation`和`Sink: Print to Std. Out`，由于上下游并行度一致且没有指定分区方式，所以这里分区方式是`FORWARD`\n\n![image-20220128104611010](../../images/flink/stream-graph-jar/stream-graph.png)\n\n\n\n最终的streamGraph为：\n\n![image-20220128104611010](../../images/flink/stream-graph-jar/stream-graph1.png)\n\n## StreamGraph生成的源码跟踪\n\n### 主流程\n\n主要包含几个步骤：\n\n1. 初始化并配置`streamGraph`的信息\n2.  遍历所有的`Transformation`,并对`transformation`进行转换\n\n```java\n//StreamGraphGenerator\npublic StreamGraph generate() {\n    \t//1. 初始化并配置streamGraph的信息\n        streamGraph = new StreamGraph(executionConfig, checkpointConfig, savepointRestoreSettings);\n        streamGraph.setEnableCheckpointsAfterTasksFinish(\n                configuration.get(\n                        ExecutionCheckpointingOptions.ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH));\n        shouldExecuteInBatchMode = shouldExecuteInBatchMode();\n        configureStreamGraph(streamGraph);\n\t\t//用户保存已经转换的Transformation\n        alreadyTransformed = new HashMap<>();\n\t\t//2. 对transformation进行转换\n        for (Transformation<?> transformation : transformations) {\n            transform(transformation);\n        }\n\t\t\n        streamGraph.setSlotSharingGroupResource(slotSharingGroupResources);\n\n        setFineGrainedGlobalStreamExchangeMode(streamGraph);\n\n        for (StreamNode node : streamGraph.getStreamNodes()) {\n            if (node.getInEdges().stream().anyMatch(this::shouldDisableUnalignedCheckpointing)) {\n                for (StreamEdge edge : node.getInEdges()) {\n                    edge.setSupportsUnalignedCheckpoints(false);\n                }\n            }\n        }\n\n        final StreamGraph builtStreamGraph = streamGraph;\n\n        alreadyTransformed.clear();\n        alreadyTransformed = null;\n        streamGraph = null;\n\n        return builtStreamGraph;\n    }\n```\n\n核心代码在`transform(transformation)`。\n\n```java\n//StreamGraphGenerator\nprivate Collection<Integer> transform(Transformation<?> transform) {\n\t//1. 如果某个transformation已经转换过，直接返回transformedId,这里要判断，是因为graph可能会出现loop\n    if (alreadyTransformed.containsKey(transform)) {\n        return alreadyTransformed.get(transform);\n    }\n\n    LOG.debug(\"Transforming \" + transform);\n    //2. 设置并行度\n    if (transform.getMaxParallelism() <= 0) {\n\n        // if the max parallelism hasn't been set, then first use the job wide max parallelism\n        // from the ExecutionConfig.\n        int globalMaxParallelismFromConfig = executionConfig.getMaxParallelism();\n        if (globalMaxParallelismFromConfig > 0) {\n            transform.setMaxParallelism(globalMaxParallelismFromConfig);\n        }\n    }\n\t//3. 设置slot共享组\n    transform\n            .getSlotSharingGroup()\n            .ifPresent(\n                    slotSharingGroup -> {\n                        final ResourceSpec resourceSpec =\n                                SlotSharingGroupUtils.extractResourceSpec(slotSharingGroup);\n                        if (!resourceSpec.equals(ResourceSpec.UNKNOWN)) {\n                            slotSharingGroupResources.compute(\n                                    slotSharingGroup.getName(),\n                                    (name, profile) -> {\n                                        if (profile == null) {\n                                            return ResourceProfile.fromResourceSpec(\n                                                    resourceSpec, MemorySize.ZERO);\n                                        } else if (!ResourceProfile.fromResourceSpec(\n                                                        resourceSpec, MemorySize.ZERO)\n                                                .equals(profile)) {\n                                            throw new IllegalArgumentException(\n                                                    \"The slot sharing group \"\n                                                            + slotSharingGroup.getName()\n                                                            + \" has been configured with two different resource spec.\");\n                                        } else {\n                                            return profile;\n                                        }\n                                    });\n                        }\n                    });\n\t\n    //4. 调用判断是否有推断出outputType，有则抛出异常\n    transform.getOutputType();\n\n    //5. 调用具体的translator\n    @SuppressWarnings(\"unchecked\")\n    final TransformationTranslator<?, Transformation<?>> translator =\n            (TransformationTranslator<?, Transformation<?>>)\n                    translatorMap.get(transform.getClass());\n\n    Collection<Integer> transformedIds;\n    if (translator != null) {\n        transformedIds = translate(translator, transform);\n    } else {\n        transformedIds = legacyTransform(transform);\n    }\n\n    // need this check because the iterate transformation adds itself before\n    // transforming the feedback edges\n    if (!alreadyTransformed.containsKey(transform)) {\n        alreadyTransformed.put(transform, transformedIds);\n    }\n\n    return transformedIds;\n}\n```\n\n步骤如下：\n\n1. 如果某个`transformation`已经转换过，直接返回`transformedId`,这里要判断，是因为`graph`可能会出现`loop`\n2. 设置并行度\n3. 设置`SlotSharingGroup`\n4. 调用判断是否有推断出outputType，有则抛出异常\n5. 调用具体的`translator`，内置的`translator`都被保留在`translatorMap`中，具体如下所示\n\n```java\n//StreamGraphGenerator\nstatic {\n    @SuppressWarnings(\"rawtypes\")\n    Map<Class<? extends Transformation>, TransformationTranslator<?, ? extends Transformation>>\n            tmp = new HashMap<>();\n    tmp.put(OneInputTransformation.class, new OneInputTransformationTranslator<>());\n    tmp.put(TwoInputTransformation.class, new TwoInputTransformationTranslator<>());\n    tmp.put(MultipleInputTransformation.class, new MultiInputTransformationTranslator<>());\n    tmp.put(KeyedMultipleInputTransformation.class, new MultiInputTransformationTranslator<>());\n    tmp.put(SourceTransformation.class, new SourceTransformationTranslator<>());\n    tmp.put(SinkTransformation.class, new SinkTransformationTranslator<>());\n    tmp.put(LegacySinkTransformation.class, new LegacySinkTransformationTranslator<>());\n    tmp.put(LegacySourceTransformation.class, new LegacySourceTransformationTranslator<>());\n    tmp.put(UnionTransformation.class, new UnionTransformationTranslator<>());\n    tmp.put(PartitionTransformation.class, new PartitionTransformationTranslator<>());\n    tmp.put(SideOutputTransformation.class, new SideOutputTransformationTranslator<>());\n    tmp.put(ReduceTransformation.class, new ReduceTransformationTranslator<>());\n    tmp.put(\n            TimestampsAndWatermarksTransformation.class,\n            new TimestampsAndWatermarksTransformationTranslator<>());\n    tmp.put(BroadcastStateTransformation.class, new BroadcastStateTransformationTranslator<>());\n    tmp.put(\n            KeyedBroadcastStateTransformation.class,\n            new KeyedBroadcastStateTransformationTranslator<>());\n    translatorMap = Collections.unmodifiableMap(tmp);\n}\n```\n\n6. 将转换过的`transformation`添加到`alreadyTransformed`中\n\n调用具体的`translator`翻译的代码逻辑如下所示\n\n```java\n//StreamGraphGenerator\nprivate Collection<Integer> translate(\n        final TransformationTranslator<?, Transformation<?>> translator,\n        final Transformation<?> transform) {\n    checkNotNull(translator);\n    checkNotNull(transform);\n\t\n    final List<Collection<Integer>> allInputIds = getParentInputIds(transform.getInputs());\n\n    // the recursive call might have already transformed this\n    if (alreadyTransformed.containsKey(transform)) {\n        return alreadyTransformed.get(transform);\n    }\n\n    final String slotSharingGroup =\n            determineSlotSharingGroup(\n                    transform.getSlotSharingGroup().isPresent()\n                            ? transform.getSlotSharingGroup().get().getName()\n                            : null,\n                    allInputIds.stream()\n                            .flatMap(Collection::stream)\n                            .collect(Collectors.toList()));\n\n    final TransformationTranslator.Context context =\n            new ContextImpl(this, streamGraph, slotSharingGroup, configuration);\n\n    return shouldExecuteInBatchMode\n            ? translator.translateForBatch(transform, context)\n            : translator.translateForStreaming(transform, context);\n}\n```\n\n下面我们针对具体的`Transformation`来进行理解。\n\n\n\n### OneInputTransformation的转换\n\n在进入`translate(translator, transform)`时， 首先看看`OneInputTransformation`的属性。\n\n![image-20220124141540261](../../images/flink/stream-graph-jar/stream-5.png)\n\n方法`getParentInputIds(transform.getInputs())`中，可以看到先对`OneInputTransformation`的输入进行转换，而它的输入为`LegacySourceTransformation`\n\n![image-20220124141925119](../../images/flink/stream-graph-jar/stream-6.png)\n\n#### 设置上游input transformation——LegacySourceTransformation的转换\n\n同样的，先对其input进行转换，由于`LegacySourceTransformation`没有input，所以对自身进行translate\n\n![image-20220124143123102](../../images/flink/stream-graph-jar/stream-7.png)\n\n具体根据`StreamGraphGenerator`中的`shouldExecuteInBatchMode`去决定转成流还是批模式的，这里是流模式\n\n```java\nreturn shouldExecuteInBatchMode\n        ? translator.translateForBatch(transform, context)\n        : translator.translateForStreaming(transform, context);\n```\n\n```java\n//SimpleTransformationTranslator\n@Override\npublic final Collection<Integer> translateForStreaming(\n        final T transformation, final Context context) {\n    checkNotNull(transformation);\n    checkNotNull(context);\n\t\n    final Collection<Integer> transformedIds =\n            translateForStreamingInternal(transformation, context);\n    configure(transformation, context);\n\n    return transformedIds;\n}\n```\n\n这里调用了`flink-stream-java`模块里的`LegacySourceTransformationTranslator`\n\n```java\n//LegacySourceTransformationTranslator\n\nprivate Collection<Integer> translateInternal(\n        final LegacySourceTransformation<OUT> transformation, final Context context) {\n    checkNotNull(transformation);\n    checkNotNull(context);\n\n    final StreamGraph streamGraph = context.getStreamGraph();\n    final String slotSharingGroup = context.getSlotSharingGroup();\n    final int transformationId = transformation.getId();\n    final ExecutionConfig executionConfig = streamGraph.getExecutionConfig();\n\n    //1. 添加source算子\n    streamGraph.addLegacySource(\n            transformationId,\n            slotSharingGroup,\n            transformation.getCoLocationGroupKey(),\n            transformation.getOperatorFactory(),\n            null,\n            transformation.getOutputType(),\n            \"Source: \" + transformation.getName());\n\n    if (transformation.getOperatorFactory() instanceof InputFormatOperatorFactory) {\n        streamGraph.setInputFormat(\n                transformationId,\n                ((InputFormatOperatorFactory<OUT>) transformation.getOperatorFactory())\n                        .getInputFormat());\n    }\n\t//设置并行度\n    final int parallelism =\n            transformation.getParallelism() != ExecutionConfig.PARALLELISM_DEFAULT\n                    ? transformation.getParallelism()\n                    : executionConfig.getParallelism();\n    streamGraph.setParallelism(transformationId, parallelism);\n    streamGraph.setMaxParallelism(transformationId, transformation.getMaxParallelism());\n\n    return Collections.singleton(transformationId);\n}\n```\n\n##### 添加source算子\n\n```java\n//StreamGraph\npublic <IN, OUT> void addLegacySource(\n        Integer vertexID,\n        @Nullable String slotSharingGroup,\n        @Nullable String coLocationGroup,\n        StreamOperatorFactory<OUT> operatorFactory,\n        TypeInformation<IN> inTypeInfo,\n        TypeInformation<OUT> outTypeInfo,\n        String operatorName) {\n    addOperator(\n            vertexID,\n            slotSharingGroup,\n            coLocationGroup,\n            operatorFactory,\n            inTypeInfo,\n            outTypeInfo,\n            operatorName);\n    sources.add(vertexID);\n}\n\npublic <IN, OUT> void addOperator(\n            Integer vertexID,\n            @Nullable String slotSharingGroup,\n            @Nullable String coLocationGroup,\n            StreamOperatorFactory<OUT> operatorFactory,\n            TypeInformation<IN> inTypeInfo,\n            TypeInformation<OUT> outTypeInfo,\n            String operatorName) {\n        Class<? extends TaskInvokable> invokableClass =\n                operatorFactory.isStreamSource()\n                        ? SourceStreamTask.class\n                        : OneInputStreamTask.class;\n        addOperator(\n                vertexID,\n                slotSharingGroup,\n                coLocationGroup,\n                operatorFactory,\n                inTypeInfo,\n                outTypeInfo,\n                operatorName,\n                invokableClass);\n    }\n\nprivate <IN, OUT> void addOperator(\n            Integer vertexID,\n            @Nullable String slotSharingGroup,\n            @Nullable String coLocationGroup,\n            StreamOperatorFactory<OUT> operatorFactory,\n            TypeInformation<IN> inTypeInfo,\n            TypeInformation<OUT> outTypeInfo,\n            String operatorName,\n            Class<? extends TaskInvokable> invokableClass) {\n\n    \t//添加StreamNode，生成streamNode并添加进Map<Integer, StreamNode>里\n        addNode(\n                vertexID,\n                slotSharingGroup,\n                coLocationGroup,\n                invokableClass,\n                operatorFactory,\n                operatorName);\n    \t//设置该transformation输入和输出的序列化方式\n        setSerializers(vertexID, createSerializer(inTypeInfo), null, createSerializer(outTypeInfo));\n\t\t//设置outputType\n        if (operatorFactory.isOutputTypeConfigurable() && outTypeInfo != null) {\n            // sets the output type which must be know at StreamGraph creation time\n            operatorFactory.setOutputType(outTypeInfo, executionConfig);\n        }\n\t\t//设置inputType\n        if (operatorFactory.isInputTypeConfigurable()) {\n            operatorFactory.setInputType(inTypeInfo, executionConfig);\n        }\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Vertex: {}\", vertexID);\n        }\n    }\n```\n\n![image-20220124152339087](../../images/flink/stream-graph-jar/stream-8.png)\n\n执行完`addOperator()`后：\n\n<img src=\"../../images/flink/stream-graph-jar/stream-9.png\" alt=\"image-20220127145023757\" style=\"zoom: 60%;\" /><img src=\"../../images/flink/stream-graph-jar/stream-10.png\" alt=\"image-20220127145613925\" style=\"zoom:70%;\" />\n\n##### 设置完并行度后\n\n![image-20220127151952235](../../images/flink/stream-graph-jar/stream-11.png)\n\n可以看到设置`parallelism=1, maxParallelism=-1`\n\n回到`SimpleTransformationTranslator.translateForStreaming()`，下一步是`configure(transformation, context);`\n\n##### 设置uid，用户提供的节点hash函数，资源等。\n\n```java\nprivate void configure(final T transformation, final Context context) {\n    final StreamGraph streamGraph = context.getStreamGraph();\n    final int transformationId = transformation.getId();\n\n    StreamGraphUtils.configureBufferTimeout(\n            streamGraph, transformationId, transformation, context.getDefaultBufferTimeout());\n\t//设置算子uid\n    if (transformation.getUid() != null) {\n        streamGraph.setTransformationUID(transformationId, transformation.getUid());\n    }\n    if (transformation.getUserProvidedNodeHash() != null) {\n        streamGraph.setTransformationUserHash(\n                transformationId, transformation.getUserProvidedNodeHash());\n    }\n\n    StreamGraphUtils.validateTransformationUid(streamGraph, transformation);\n\t//设置资源和验证\n    if (transformation.getMinResources() != null\n            && transformation.getPreferredResources() != null) {\n        streamGraph.setResources(\n                transformationId,\n                transformation.getMinResources(),\n                transformation.getPreferredResources());\n    }\n\n    final StreamNode streamNode = streamGraph.getStreamNode(transformationId);\n    if (streamNode != null) {\n        validateUseCaseWeightsNotConflict(\n                streamNode.getManagedMemoryOperatorScopeUseCaseWeights(),\n                transformation.getManagedMemoryOperatorScopeUseCaseWeights());\n        streamNode.setManagedMemoryUseCaseWeights(\n                transformation.getManagedMemoryOperatorScopeUseCaseWeights(),\n                transformation.getManagedMemorySlotScopeUseCases());\n    }\n}\n```\n\n至此完成`LegacySourceTransformation`的转换，回到`OneInputTransformation`的转换\n\n```java\nprotected Collection<Integer> translateInternal(\n        final Transformation<OUT> transformation,\n        final StreamOperatorFactory<OUT> operatorFactory,\n        final TypeInformation<IN> inputType,\n        @Nullable final KeySelector<IN, ?> stateKeySelector,\n        @Nullable final TypeInformation<?> stateKeyType,\n        final Context context) {\n    checkNotNull(transformation);\n    checkNotNull(operatorFactory);\n    checkNotNull(inputType);\n    checkNotNull(context);\n\n    final StreamGraph streamGraph = context.getStreamGraph();\n    final String slotSharingGroup = context.getSlotSharingGroup();\n    final int transformationId = transformation.getId();\n    final ExecutionConfig executionConfig = streamGraph.getExecutionConfig();\n\t//往streamGraph中添加算子\n    streamGraph.addOperator(\n            transformationId,\n            slotSharingGroup,\n            transformation.getCoLocationGroupKey(),\n            operatorFactory,\n            inputType,\n            transformation.getOutputType(),\n            transformation.getName());\n\t//设置该算子的状态key\n    if (stateKeySelector != null) {\n        TypeSerializer<?> keySerializer = stateKeyType.createSerializer(executionConfig);\n        streamGraph.setOneInputStateKey(transformationId, stateKeySelector, keySerializer);\n    }\n\t//设置并行度\n    int parallelism =\n            transformation.getParallelism() != ExecutionConfig.PARALLELISM_DEFAULT\n                    ? transformation.getParallelism()\n                    : executionConfig.getParallelism();\n    streamGraph.setParallelism(transformationId, parallelism);\n    streamGraph.setMaxParallelism(transformationId, transformation.getMaxParallelism());\n\t//找到父transformation\n    final List<Transformation<?>> parentTransformations = transformation.getInputs();\n    checkState(\n            parentTransformations.size() == 1,\n            \"Expected exactly one input transformation but found \"\n                    + parentTransformations.size());\n\t//添加streamEdge\n    for (Integer inputId : context.getStreamNodeIds(parentTransformations.get(0))) {\n        streamGraph.addEdge(inputId, transformationId, 0);\n    }\n\n    return Collections.singleton(transformationId);\n}\n```\n\n![image-20220127154909615](../../images/flink/stream-graph-jar/stream-12.png)\n\n和`LegacySourceTransformation`类似，经过\n\n- 添加`StreamNode`\n- 设置该算子的状态key\n- 设置并行度\n- 设置uid、资源等\n\n`streamGraph`中的`streamNode`为\n\n<img src=\"../../images/flink/stream-graph-jar/stream-13.png\" alt=\"image-20220127155951663\" style=\"zoom:80%;\" />\n\n#### 添加streamEdge\n\n```java\n//StreamGraph\npublic void addEdge(Integer upStreamVertexID, Integer downStreamVertexID, int typeNumber) {\n    addEdgeInternal(\n            upStreamVertexID,\n            downStreamVertexID,\n            typeNumber,\n            null,\n            new ArrayList<String>(),\n            null,\n            null);\n}\n\nprivate void addEdgeInternal(\n            Integer upStreamVertexID,\n            Integer downStreamVertexID,\n            int typeNumber,\n            StreamPartitioner<?> partitioner,\n            List<String> outputNames,\n            OutputTag outputTag,\n            StreamExchangeMode exchangeMode) {\n\n        if (virtualSideOutputNodes.containsKey(upStreamVertexID)) {\n            int virtualId = upStreamVertexID;\n            upStreamVertexID = virtualSideOutputNodes.get(virtualId).f0;\n            if (outputTag == null) {\n                outputTag = virtualSideOutputNodes.get(virtualId).f1;\n            }\n            addEdgeInternal(\n                    upStreamVertexID,\n                    downStreamVertexID,\n                    typeNumber,\n                    partitioner,\n                    null,\n                    outputTag,\n                    exchangeMode);\n        } else if (virtualPartitionNodes.containsKey(upStreamVertexID)) {\n            int virtualId = upStreamVertexID;\n            upStreamVertexID = virtualPartitionNodes.get(virtualId).f0;\n            if (partitioner == null) {\n                partitioner = virtualPartitionNodes.get(virtualId).f1;\n            }\n            exchangeMode = virtualPartitionNodes.get(virtualId).f2;\n            addEdgeInternal(\n                    upStreamVertexID,\n                    downStreamVertexID,\n                    typeNumber,\n                    partitioner,\n                    outputNames,\n                    outputTag,\n                    exchangeMode);\n        } else {\n            //添加实际的Edge\n            createActualEdge(\n                    upStreamVertexID,\n                    downStreamVertexID,\n                    typeNumber,\n                    partitioner,\n                    outputTag,\n                    exchangeMode);\n        }\n    }\n\nprivate void createActualEdge(\n            Integer upStreamVertexID,\n            Integer downStreamVertexID,\n            int typeNumber,\n            StreamPartitioner<?> partitioner,\n            OutputTag outputTag,\n            StreamExchangeMode exchangeMode) {\n        StreamNode upstreamNode = getStreamNode(upStreamVertexID);\n        StreamNode downstreamNode = getStreamNode(downStreamVertexID);\n\n        //如果没有指定分区器和上下行并行度, 操作符匹配使用forward分区策略，否则使用rebalance。\n        if (partitioner == null\n                && upstreamNode.getParallelism() == downstreamNode.getParallelism()) {\n            partitioner = new ForwardPartitioner<Object>();\n        } else if (partitioner == null) {\n            partitioner = new RebalancePartitioner<Object>();\n        }\n\n        ...\n\n        if (exchangeMode == null) {\n            exchangeMode = StreamExchangeMode.UNDEFINED;\n        }\n\n        /**\n         * Just make sure that {@link StreamEdge} connecting same nodes (for example as a result of\n         * self unioning a {@link DataStream}) are distinct and unique. Otherwise it would be\n         * difficult on the {@link StreamTask} to assign {@link RecordWriter}s to correct {@link\n         * StreamEdge}.\n         */\n        int uniqueId = getStreamEdges(upstreamNode.getId(), downstreamNode.getId()).size();\n\n        StreamEdge edge =\n                new StreamEdge(\n                        upstreamNode,\n                        downstreamNode,\n                        typeNumber,\n                        partitioner,\n                        outputTag,\n                        exchangeMode,\n                        uniqueId);\n\t\t//添加streamEdge到对应的streamNode中\n        getStreamNode(edge.getSourceId()).addOutEdge(edge);\n        getStreamNode(edge.getTargetId()).addInEdge(edge);\n    }\n```\n\n以上在`LegacySourceTransformation`生成的`streamNode`和`OneInputTransformation`生成的`streamNode`中用`streamEdge`连起来。\n\n![image-20220127171357682](../../images/flink/stream-graph-jar/stream-14.png)\n\n### ReduceTransformation的转换\n\n先处理input，input为`PartitionTransformation`\n\n#### PartitionTransformation的转换\n\n- 找到Input\n- 添加一个虚拟分区节点，不会生成 `StreamNode`，将PartitionTransformation的相关信息保存到`Map<Integer, Tuple3<Integer, StreamPartitioner<?>, StreamExchangeMode>> virtualPartitionNodes`中，`virtualPartitionNodes`的key是 虚拟分区节点的id, Tuple3中的第一个参数是input的id\n\n```java\nprivate Collection<Integer> translateInternal(\n        final PartitionTransformation<OUT> transformation, final Context context) {\n    checkNotNull(transformation);\n    checkNotNull(context);\n\n    final StreamGraph streamGraph = context.getStreamGraph();\n\n    final List<Transformation<?>> parentTransformations = transformation.getInputs();\n    checkState(\n            parentTransformations.size() == 1,\n            \"Expected exactly one input transformation but found \"\n                    + parentTransformations.size());\n    final Transformation<?> input = parentTransformations.get(0);\n\n    List<Integer> resultIds = new ArrayList<>();\n\n    for (Integer inputId : context.getStreamNodeIds(input)) {\n        final int virtualId = Transformation.getNewNodeId();\n        //添加一个虚拟分区节点，不会生成 StreamNode\n        streamGraph.addVirtualPartitionNode(\n                inputId,\n                virtualId,\n                transformation.getPartitioner(),\n                transformation.getExchangeMode());\n        resultIds.add(virtualId);\n    }\n    return resultIds;\n}\n```\n\n![image-20220128090842795](../../images/flink/stream-graph-jar/sream-15.png)\n\n完成了`PartitionTransformation`的转换后，进入`ReduceTransformation`\n\n```java\n//ReduceTransformationTranslator\n\n@Override\npublic Collection<Integer> translateForStreamingInternal(\n        final ReduceTransformation<IN, KEY> transformation, final Context context) {\n    StreamGroupedReduceOperator<IN> groupedReduce =\n            new StreamGroupedReduceOperator<>(\n                    transformation.getReducer(),\n                    transformation\n                            .getInputType()\n                            .createSerializer(context.getStreamGraph().getExecutionConfig()));\n\n    SimpleOperatorFactory<IN> operatorFactory = SimpleOperatorFactory.of(groupedReduce);\n    operatorFactory.setChainingStrategy(transformation.getChainingStrategy());\n    return translateInternal(\n            transformation,\n            operatorFactory,\n            transformation.getInputType(),\n            transformation.getKeySelector(),\n            transformation.getKeyTypeInfo(),\n            context);\n}\n```\n\n同样的，先生成StreamNode，然后添加streamEdge。生成StreamEdge和之前的略有不同，因为`ReduceTransformation`的上游是`PartitionTransformation`，而`PartitionTransformation`是虚拟分区节点。\n\n```java\n} else if (virtualPartitionNodes.containsKey(upStreamVertexID)) {\n    int virtualId = upStreamVertexID;\n    upStreamVertexID = virtualPartitionNodes.get(virtualId).f0;\n    if (partitioner == null) {\n        partitioner = virtualPartitionNodes.get(virtualId).f1;\n    }\n    exchangeMode = virtualPartitionNodes.get(virtualId).f2;\n    addEdgeInternal(\n            upStreamVertexID,\n            downStreamVertexID,\n            typeNumber,\n            partitioner,\n            outputNames,\n            outputTag,\n            exchangeMode);\n}\n```\n\n![image-20220128094222860](../../images/flink/stream-graph-jar/stream-16.png)\n\n过程是这样的：\n\n- `OneInputTransformation`生成的`StreamNode `Id=2, `PartitionTransformation`生成的虚拟分区节点(`VirtualPartitionNode`)的`virtualId=6`，`ReduceTransformation`的`StreamNode `Id=4\n\n- 进入`ReduceTransformation`转换的`streamGraph.addEdgeInternal`方法后，发现`ReduceTransformation`的上游的id在`virtualPartitionNodes`中存在，便从`virtualPartitionNodes`拿到虚拟分区节点的上游id、`StreamPartitioner`和`StreamExchangeMode`\n\n- 递归调用`streamGraph.addEdgeInternal`，将`outputPartitioner`和`exchangeMode`等信息保存到streamEdge中，并添加到`OneInputTransformation`生成的`StreamNode `（Flat Map-2）的`outEdges`和`ReduceTransformation`生成的`StreamNode`(Keyed Aggregation-4)的`inEdges`中。\n\n![image-20220128101904692](../../images/flink/stream-graph-jar/stream-15.png)\n\n至此完成了`ReduceTransformation`的转换，其`streamNode`如下图所示。\n\n![image-20220128104305244](../../images/flink/stream-graph-jar/stream-17.png)\n\n### LegacySinkTransformation的转换\n\n流程和其他Transformation一样\n\n```java\n//LegacySinkTransformationTranslator\nprivate Collection<Integer> translateInternal(\n        final LegacySinkTransformation<IN> transformation, final Context context) {\n    checkNotNull(transformation);\n    checkNotNull(context);\n\n    final StreamGraph streamGraph = context.getStreamGraph();\n    final String slotSharingGroup = context.getSlotSharingGroup();\n    final int transformationId = transformation.getId();\n    final ExecutionConfig executionConfig = streamGraph.getExecutionConfig();\n\n    final List<Transformation<?>> parentTransformations = transformation.getInputs();\n    checkState(\n            parentTransformations.size() == 1,\n            \"Expected exactly one input transformation but found \"\n                    + parentTransformations.size());\n    final Transformation<?> input = parentTransformations.get(0);\n\t//添加sink节点\n    streamGraph.addSink(\n            transformationId,\n            slotSharingGroup,\n            transformation.getCoLocationGroupKey(),\n            transformation.getOperatorFactory(),\n            input.getOutputType(),\n            null,\n            \"Sink: \" + transformation.getName());\n\n    final int parallelism =\n            transformation.getParallelism() != ExecutionConfig.PARALLELISM_DEFAULT\n                    ? transformation.getParallelism()\n                    : executionConfig.getParallelism();\n    streamGraph.setParallelism(transformationId, parallelism);\n    streamGraph.setMaxParallelism(transformationId, transformation.getMaxParallelism());\n\n    for (Integer inputId : context.getStreamNodeIds(input)) {\n        streamGraph.addEdge(inputId, transformationId, 0);\n    }\n\n    if (transformation.getStateKeySelector() != null) {\n        TypeSerializer<?> keySerializer =\n                transformation.getStateKeyType().createSerializer(executionConfig);\n        streamGraph.setOneInputStateKey(\n                transformationId, transformation.getStateKeySelector(), keySerializer);\n    }\n\n    return Collections.emptyList();\n}\n```\n\n```java\n//StreamGraph\npublic <IN, OUT> void addSink(\n        Integer vertexID,\n        @Nullable String slotSharingGroup,\n        @Nullable String coLocationGroup,\n        StreamOperatorFactory<OUT> operatorFactory,\n        TypeInformation<IN> inTypeInfo,\n        TypeInformation<OUT> outTypeInfo,\n        String operatorName) {\n    addOperator(\n            vertexID,\n            slotSharingGroup,\n            coLocationGroup,\n            operatorFactory,\n            inTypeInfo,\n            outTypeInfo,\n            operatorName);\n    if (operatorFactory instanceof OutputFormatOperatorFactory) {\n        setOutputFormat(\n                vertexID, ((OutputFormatOperatorFactory) operatorFactory).getOutputFormat());\n    }\n    sinks.add(vertexID);\n}\n```\n\n具体逻辑和`LegacySourceTransformation`差不多。\n\n![image-20220128104611010](../../images/flink/stream-graph-jar/stream-18.png)\n\n","tags":["flink"],"categories":["flink"]},{"title":"flink作业提交源码解析(1)","url":"/flink/flink-client/","content":"# flink作业提交源码解析（1）-命令行解析及运行\n\n从`bin/flink` 这个提交脚本最后一行\n```shell\nexec \"${JAVA_RUN}\" $JVM_ARGS $FLINK_ENV_JAVA_OPTS \"${log_setting[@]}\" -classpath \"`manglePathList \"$CC_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS\"`\" org.apache.flink.client.cli.CliFrontend \"$@\"\n```\n可以看出，入口类为`org.apache.flink.client.cli.CliFrontend`。\n\n[开启远程调试功能](remote-debug.md),在`org.apache.flink.client.cli.CliFrontend`的main函数中打断点。\n\n## 总流程\n\nflink-client 入口: CliFrontend.java\n```java\n /** Submits the job based on the arguments. */\n    public static void main(final String[] args) {\n        //加载环境变量，包括 code revision, current user, Java version, Hadoop version, JVM parameters\n        EnvironmentInformation.logEnvironmentInfo(LOG, \"Command Line Client\", args);\n\n        // 1. find the configuration directory\n        final String configurationDirectory = getConfigurationDirectoryFromEnv();\n\n        // 2. load the global configuration\n        final Configuration configuration =\n                GlobalConfiguration.loadConfiguration(configurationDirectory);\n\n        // 3. load the custom command lines\n        final List<CustomCommandLine> customCommandLines =\n                loadCustomCommandLines(configuration, configurationDirectory);\n\n        int retCode = 31;\n        try {\n            final CliFrontend cli = new CliFrontend(configuration, customCommandLines);\n\n            SecurityUtils.install(new SecurityConfiguration(cli.configuration));\n            retCode = SecurityUtils.getInstalledContext().runSecured(() -> cli.parseAndRun(args));\n        } catch (Throwable t) {\n            final Throwable strippedThrowable =\n                    ExceptionUtils.stripException(t, UndeclaredThrowableException.class);\n            LOG.error(\"Fatal error while running command line interface.\", strippedThrowable);\n            strippedThrowable.printStackTrace();\n        } finally {\n            System.exit(retCode);\n        }\n    }\n```\n\n主要分为4个步骤：\n\n1. 加载环境变量，包括 code revision, current user, Java version, Hadoop version, JVM parameters等，然后从环境变量中获取到flink的配置目录\n2. 解析配置，读取``flink-conf.yaml`，如下图所示\n\n![image-20220123145950769](../../images/flink/flink-client/client.png)\n\n3. 初始化命令行解析入口，这里包含了一下三种命令行解析入口类\n\n   ```java\n   //运行bin/flink run-application xx 或者bin/flink run -t xxx的 ，\n   //-t包含了\"remote\", \"local\", \"kubernetes-session\",\"yarn-per-job\", \"yarn-session\"等模式\n   org.apache.flink.client.cli.GenericCLI\n   //提交到yarn session集群的命令行解析入口\n   org.apache.flink.yarn.cli.FlinkYarnSessionCli\n   //默认的命令行解析入口类，提交到standalone集群\n   org.apache.flink.client.cli.DefaultCLI\n   ```\n\n4. 初始化`CliFrontend`,执行`cli.parseAndRun(args)`，完成flink client的主要逻辑，其中我们最需要关注的，便是`cli.parseAndRun(args)`做了些什么事情。\n\n## 命令行解析和运行\n\n进入`parseAndRun`函数中，可以看到程序通过解析第一个参数，来判断进入进入哪种具体的逻辑，然后把args[1:]的作为参数`params`传入具体函数里。这里我们使用第一种，即进入`run(params)`中。\n\n```java\npublic int parseAndRun(String[] args) {\n        // check for action\n        if (args.length < 1) {\n            CliFrontendParser.printHelp(customCommandLines);\n            System.out.println(\"Please specify an action.\");\n            return 1;\n        }\n        // get action\n        String action = args[0];\n        // remove action from parameters\n        final String[] params = Arrays.copyOfRange(args, 1, args.length);\n        try {\n            // do action\n            switch (action) {\n                case ACTION_RUN:\n                    run(params);\n                    return 0;\n                case ACTION_RUN_APPLICATION:\n                    runApplication(params);\n                    return 0;\n                case ACTION_LIST:\n                    list(params);\n                    return 0;\n                case ACTION_INFO:\n                    info(params);\n                    return 0;\n                case ACTION_CANCEL:\n                    cancel(params);\n                    return 0;\n                case ACTION_STOP:\n                    stop(params);\n                    return 0;\n                case ACTION_SAVEPOINT:\n                    savepoint(params);\n                    return 0;\n   ...\n```\n\n```java\nprotected void run(String[] args) throws Exception {\n    LOG.info(\"Running 'run' command.\");\n\t//1. 获取支持的参数\n    final Options commandOptions = CliFrontendParser.getRunCommandOptions();\n    //2. 解析args，封装到命令行工具类里\n    final CommandLine commandLine = getCommandLine(commandOptions, args, true);\n\n    // evaluate help flag\n    if (commandLine.hasOption(HELP_OPTION.getOpt())) {\n        CliFrontendParser.printHelpForRun(customCommandLines);\n        return;\n    }\n\n    final CustomCommandLine activeCommandLine =\n            validateAndGetActiveCommandLine(checkNotNull(commandLine));\n\t//3. 使用解析到的commandLine去生成ProgramOptions\n    final ProgramOptions programOptions = ProgramOptions.create(commandLine);\n\t//4. 提取作业的入口jar包\n    final List<URL> jobJars = getJobJarAndDependencies(programOptions);\n\t//5. 提取有效的配置\n    final Configuration effectiveConfiguration =\n            getEffectiveConfiguration(activeCommandLine, commandLine, programOptions, jobJars);\n\n    LOG.debug(\"Effective executor configuration: {}\", effectiveConfiguration);\n\t//6. 构建PackagedProgram\n    try (PackagedProgram program = getPackagedProgram(programOptions, effectiveConfiguration)) {\n        //7. 执行用户jar包逻辑以及提交作业\n        executeProgram(effectiveConfiguration, program);\n    }\n}\n```\n\nrun函数包含了以下步骤\n\n1. 获取`DefaultCLI`这个命令行解析入口类支持的参数，包含以下参数，所有的参数都封装在`org.apache.commons.cli.Options`里。\n\n![image-20220123154347506](../../images/flink/flink-client/client-1.png)\n\n2. 解析args，将解析结果封装到命令行工具类里，如下图\n\n```java\nbin/flink run -d -p 1 -m 10.219.57.87:8081  \\\n-C file:/home/jinyaqia/blink-test/flink-connector-kafka_2.12-1.14.2.jar \\\n-C file:/home/jinyaqia/blink-test/flink-text-0.1.0-SNAPSHOT.jar \\\n/home/jinyaqia/blink-test/flink-sql-submit-0.1.0-SNAPSHOT.jar\n-name query_6311_dw_liangyu_211112104148\n-f /home/jinyaqia/blink-test/114kafka.sql\n```\n\n![image-20220123155127326](../../images/flink/flink-client/client-2.png)\n\n3. 生成`ProgramOptions`\n\n   ```\n   /** Base class for command line options that refer to a JAR file program. */\n   public class ProgramOptions extends CommandLineOptions {\n   \t//用户入口jar包\n       private String jarFilePath;\n   \t//用户入口类，可以用-c指定，也可以打包入口jar包的时候，指定入口类\n       protected String entryPointClass;\n   \t//通过-C指定的classpath\n       private final List<URL> classpaths;\n   \t//用户入口类的参数\n       private final String[] programArgs;\n   \t//-p指定的并行度\n       private final int parallelism;\n   \t//用-d表示detached模式,即提交作业后，退出client\n       private final boolean detachedMode;\n       private final boolean shutdownOnAttachedExit;\n   \t//savapoint设置\n       private final SavepointRestoreSettings savepointSettings;\n   ```\n\n   ![image-20220123160907260](../../images/flink/flink-client/client-3.png)\n\n4. 提取作业的入口jar包\n\n5. 提取有效的配置到`Configuration`\n\n   ![image-20220123162346128](../../images/flink/flink-client/client-4.png)\n\n6. 构建`PackagedProgram`,该类用于描述用户提交的作业及其依赖的文件等。\n\n```java\n//包含以下属性\n//用户入口jar包\nprivate final URL jarFile;\n//入口jar包的参数\nprivate final String[] args;\n//入口类\nprivate final Class<?> mainClass;\n//其他资源文件\nprivate final List<File> extractedTempLibraries;\n//-C传入的classpath\nprivate final List<URL> classpaths;\n//用户类加载器\nprivate final URLClassLoader userCodeClassLoader;\n//savepoint设置\nprivate final SavepointRestoreSettings savepointSettings;\n```\n\n![image-20220123163132191](../../images/flink/flink-client/client-5.png)\n\n7. 执行用户jar包代码逻辑以及提交作业\n\n   ```java\n   protected void executeProgram(final Configuration configuration, final PackagedProgram program)\n           throws ProgramInvocationException {\n       ClientUtils.executeProgram(\n               new DefaultExecutorServiceLoader(), configuration, program, false, false);\n   }\n   ```\n\n   具体的执行逻辑如下：\n\n   - 从`PackagedProgram`中获取用于加载用户代码的类加载器，这里是`ChildFirstClassLoader`，该类可以在flink-conf.yaml中配置`classloader.resolve-order`,默认为`child-first`\n   - 设置当前执行线程的`classloader`为`ChildFirstClassLoader`\n   - 使用当前类加载器和配置去初始化`ContextEnvironment`和`StreamContextEnvironment`，这两种`ExecutionEnvironment`在运行用户代码的时候会用到，用户代码中的 getExecutionEnvironment 会返回该 Environment\n   - 运行用户代码\n- 重设为原来的`classloader`，即`AppClassLoader`\n   \n   ```java\n   //ClientUtils.java\n   public static void executeProgram(\n           PipelineExecutorServiceLoader executorServiceLoader,\n           Configuration configuration,\n           PackagedProgram program,\n           boolean enforceSingleJobExecution,\n           boolean suppressSysout)\n           throws ProgramInvocationException {\n       checkNotNull(executorServiceLoader);\n       final ClassLoader userCodeClassLoader = program.getUserCodeClassLoader();\n       final ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();\n       try {\n           Thread.currentThread().setContextClassLoader(userCodeClassLoader);\n   \n           LOG.info(\n                   \"Starting program (detached: {})\",\n                   !configuration.getBoolean(DeploymentOptions.ATTACHED));\n   \t\t//用户代码中的 getExecutionEnvironment 会返回该 Environment\n           ContextEnvironment.setAsContext(\n                   executorServiceLoader,\n                   configuration,\n                   userCodeClassLoader,\n                   enforceSingleJobExecution,\n                   suppressSysout);\n   \n           StreamContextEnvironment.setAsContext(\n                   executorServiceLoader,\n                   configuration,\n                   userCodeClassLoader,\n                   enforceSingleJobExecution,\n                   suppressSysout);\n   \n           try {\n               program.invokeInteractiveModeForExecution();\n           } finally {\n               ContextEnvironment.unsetAsContext();\n               StreamContextEnvironment.unsetAsContext();\n           }\n       } finally {\n           Thread.currentThread().setContextClassLoader(contextClassLoader);\n       }\n   }\n   ```\n\n## 用户代码的运行\n\n在运行用户代码的时候，初始化了`ContextEnvironment`和`StreamContextEnvironment`。调用`callMainMethod`方法。\n\n```java\n/**\n * This method assumes that the context environment is prepared, or the execution will be a\n * local execution by default.\n */\npublic void invokeInteractiveModeForExecution() throws ProgramInvocationException {\n    FlinkSecurityManager.monitorUserSystemExitForCurrentThread();\n    try {\n        callMainMethod(mainClass, args);\n    } finally {\n        FlinkSecurityManager.unmonitorUserSystemExitForCurrentThread();\n    }\n}\n```\n`callMainMethod`使用了反射机制，去运行用户代码的入口类。\n\n```java\nprivate static void callMainMethod(Class<?> entryClass, String[] args)\n        throws ProgramInvocationException {\n    Method mainMethod;\n    if (!Modifier.isPublic(entryClass.getModifiers())) {\n        throw new ProgramInvocationException(\n                \"The class \" + entryClass.getName() + \" must be public.\");\n    }\n\n    try {\n        mainMethod = entryClass.getMethod(\"main\", String[].class);\n    } catch (NoSuchMethodException e) {\n        throw new ProgramInvocationException(\n                \"The class \" + entryClass.getName() + \" has no main(String[]) method.\");\n    } catch (Throwable t) {\n        throw new ProgramInvocationException(\n                \"Could not look up the main(String[]) method from the class \"\n                        + entryClass.getName()\n                        + \": \"\n                        + t.getMessage(),\n                t);\n    }\n\n    if (!Modifier.isStatic(mainMethod.getModifiers())) {\n        throw new ProgramInvocationException(\n                \"The class \" + entryClass.getName() + \" declares a non-static main method.\");\n    }\n    if (!Modifier.isPublic(mainMethod.getModifiers())) {\n        throw new ProgramInvocationException(\n                \"The class \" + entryClass.getName() + \" declares a non-public main method.\");\n    }\n\n    try {\n        mainMethod.invoke(null, (Object) args);\n    } catch (IllegalArgumentException e) {\n        throw new ProgramInvocationException(\n                \"Could not invoke the main method, arguments are not matching.\", e);\n    } catch (IllegalAccessException e) {\n        throw new ProgramInvocationException(\n                \"Access to the main method was denied: \" + e.getMessage(), e);\n    } catch (InvocationTargetException e) {\n        Throwable exceptionInMethod = e.getTargetException();\n        if (exceptionInMethod instanceof Error) {\n            throw (Error) exceptionInMethod;\n        } else if (exceptionInMethod instanceof ProgramParametrizationException) {\n            throw (ProgramParametrizationException) exceptionInMethod;\n        } else if (exceptionInMethod instanceof ProgramInvocationException) {\n            throw (ProgramInvocationException) exceptionInMethod;\n        } else {\n            throw new ProgramInvocationException(\n                    \"The main method caused an error: \" + exceptionInMethod.getMessage(),\n                    exceptionInMethod);\n        }\n    } catch (Throwable t) {\n        throw new ProgramInvocationException(\n                \"An error occurred while invoking the program's main method: \" + t.getMessage(),\n                t);\n    }\n}\n```\n\n## 参考\n\n[Flink Client 实现原理与源码解析（保姆级教学）](https://cloud.tencent.com/developer/article/1786426)","tags":["flink"],"categories":["flink"]},{"title":"flink远程调试","url":"/flink/remote-debug/","content":"# flink远程调试\n\n## 背景\n\n在读flink源码的时候，如果需要了解作业运行的逻辑，除了运行单元测试之外，往往需要用到远程调试，下面就以作业提交的流程(熟悉flink client模块)来配置下远程调试的环境。\n\n```\nflink: release-1.14\nos: ubuntu 16.04\nIDE: IDEA\n```\n\n<!--more -->\n\n## 下载并编译源码\n\n首先从github上把flink源码clone下来，我这里使用的是release-1.14这个分支的代码，导入idea，在maven插件这里勾选以下的设置\n\n![image-20220123120846080](../../images/flink/remote-debug.md/debug-3.png)\n\n在`Flink:`这个模块里的`Lifecycle`点击运行`package`，也可以手动修改`flink-parent[package]`，再运行\n\n![image-20220123121043735](../../images/flink/remote-debug.md/debug-4.png)\n\n编译过程中可能会存在找不到包的情况，我使用的是阿里云的maven镜像仓库，本地编译完成后，编译完后的二进制包如下：\n\n![image-20220123115658062](../../images/flink/remote-debug.md/debug-1.png)\n\n将该包打包上传到服务器上\n\n![image-20220123115925969](../../images/flink/remote-debug.md/debug-2.png)\n\n## 启动测试集群\n\n```shell\nbin/jobmanager.sh start\nbin/taskmanager.sh start\nbin/taskmanager.sh start\n```\n\n## 修改作业提交脚本\n\n修改`bin/flink`脚本：\n\n主要是添加`JVM_REMOTE_DEBUG_ARGS`到JVM启动命令中\n\n```shell\nJVM_REMOTE_DEBUG_ARGS='-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5006'\n#JVM_REMOTE_DEBUG_ARGS=''\n# Add Client-specific JVM options\nFLINK_ENV_JAVA_OPTS=\"${FLINK_ENV_JAVA_OPTS} ${FLINK_ENV_JAVA_OPTS_CLI} ${JVM_REMOTE_DEBUG_ARGS}\"\n\n# Add HADOOP_CLASSPATH to allow the usage of Hadoop file systems\nexec \"${JAVA_RUN}\" $JVM_ARGS $FLINK_ENV_JAVA_OPTS \"${log_setting[@]}\" -classpath \"`manglePathList \"$CC_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS\"`\" org.apache.flink.client.cli.CliFrontend \"$@\"\n\n```\n\n## 运行调试作业\n\n启动命令如下：\n\n```\n/data/app/sailfish-interface-machine/platform_flink/flink_lib/flink-1.14/bin/flink run -d -p 1 -m 10.219.57.87:8081  \\\n-C file:/home/jinyaqia/blink-test/flink-connector-kafka_2.12-1.14.2.jar \\\n-C file:/home/jinyaqia/blink-test/flink-text-0.1.0-SNAPSHOT.jar \\\n/home/jinyaqia/blink-test/flink-sql-submit-0.1.0-SNAPSHOT.jar\n-name test_sql\n-f /home/jinyaqia/blink-test/114kafka.sql -explain\n```\n\n其中`flink-sql-submit-0.1.0-SNAPSHOT.jar`是我的一个jar包，用于把纯sql语句解析到`TableEnvironment`，代码已经放[Github](https://github.com/jinyaqia/flink-sql-submit)上了，感兴趣可以自取。\n\n运行完启动命令后，终端会出现\n\n```shell\nListening for transport dt_socket at address: 5006\n```\n\n等待客户端连接\n\n在IDEA新建一个`Remote JVM Debug`的配置，填写Host和端口号，`-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5006`就是添加到远程jvm环境的启动参数上的。\n\n选择要调试的模块，并在代码上打断点，运行即可。\n\n![image-20220123122327329](../../images/flink/remote-debug.md/debug-5.png)\n\n","tags":["flink"],"categories":["flink"]},{"title":"flink单独调整source并行度","url":"/flink/support-source/","content":"\n# 单独调整source并行度\n\n当前flink 1.12版本会默认将TableSourceScan算子（从kafka/pulsar/..读取数据）和Calc（反序列化）chain在一起，以降低网络io。在一些特定场景下，比如作业并行度已经调整到和topic分区数一样后仍出现积压，此时只能通过扩topic分区，再扩flink作业并行度来提高整个作业的吞吐。当flink读取那个topic有多个消费者的时候，可能会影响其他的作业，因此比较好的做法是单独设置TableSourceScan的并行度，TableSourceScan将读取的数据rebalance/rescale到下游算子，下游算子可以设置较大的并行度。\n\n可以通过给source table的ddl中添加 source.parallelism=<topic分区数>，调大作业并行度来进行优化。\n\n<!-- more -->\n## 优化前\n\n针对下面的sql\n\n```sql\ncreate view oriData (ip,ts,data) as\nselect \nip,\n`timestamp` as ts,\ndata \nfrom r_dataplat.ods_hadoop_nn_log \nwhere `module`='hdfs' and `type`='nn' and (`data` like '%completeFile%' or `data` like '%cmd=rename%' or `data` like '%cmd=delete%');\n\ncreate view parse_fileclose_rs (nn,ts,path,event_type) as\nselect ip nn,\nts,\nSUBSTRING(data from POSITION('completeFile' in data)+CHAR_LENGTH('completeFile: ') for (POSITION(' is closed ' in data)-POSITION('completeFile' in data)-CHAR_LENGTH('completeFile: ')) ) path,\n'completeFile' event_type \nfrom oriData a \nwhere `data` like '%completeFile%'\n;\n\ninsert into r_dataplat.hdfs_namenode_nodeevent(namenode,node_id,path,event_type,ts)\nselect nn as namenode,\ncast( 0 as BIGINT) node_id,\npath,\nevent_type,\nts from\nparse_fileclose_rs where POSITION('/.' in path)=0;\n--去掉隐藏文件a\n\n\ncreate view parse_filerename_rs (nn,ts,path,event_type) as\nselect ip nn ,\nts,\nSUBSTRING(data from POSITION('cmd=rename' in data)+CHAR_LENGTH('cmd=rename') for (POSITION('perm=' in data)-POSITION('cmd=rename' in data)-CHAR_LENGTH('cmd=rename')) ) path,\n'renameFile' event_type \nfrom oriData a \nwhere `data` like '%cmd=rename%'\n;\n\ninsert into r_dataplat.hdfs_namenode_nodeevent(namenode,node_id,path,event_type,ts)\nselect nn as namenode,\ncast( 0 as BIGINT) node_id,\npath,\nevent_type,\nts from\nparse_filerename_rs;\n--去掉隐藏文件\n\n\ncreate view parse_filedelete_rs (nn,ts,path,event_type) as\nselect ip nn,\nts,\nSUBSTRING(data from POSITION('cmd=delete' in data)+CHAR_LENGTH('cmd=delete') for (POSITION('perm=' in data)-POSITION('cmd=delete' in data)-CHAR_LENGTH('cmd=delete')) ) path,\n'deleteFile' event_type \nfrom oriData a \nwhere `data` like '%cmd=delete%'\n;\n\n\ninsert into r_dataplat.hdfs_namenode_nodeevent(namenode,node_id,path,event_type,ts)\nselect nn as namenode,\ncast( 0 as BIGINT) node_id,\npath,\nevent_type,\nts from\nparse_filedelete_rs where POSITION('/.' in path)=0;\n--去掉隐藏文件\n\n```\n\n生成的物理执行计划如下：\n\n```\n== Physical Execution Plan ==\nStage 1 : Data Source\n\tcontent : Source: TableSourceScan(table=[[default_catalog, default_database, ods_hadoop_nn_log]], fields=[app, module, type, data, ip, logFile, timestamp])\n\n\tStage 2 : Operator\n\t\tcontent : Calc(select=[ip, timestamp AS ts, data], where=[((CAST(module) = _UTF-16LE'hdfs':VARCHAR(4) CHARACTER SET \"UTF-16LE\") AND (CAST(type) = _UTF-16LE'nn':VARCHAR(2) CHARACTER SET \"UTF-16LE\") AND ((data LIKE _UTF-16LE'%completeFile%') OR (data LIKE _UTF-16LE'%cmd=rename%') OR (data LIKE _UTF-16LE'%cmd=delete%')))])\n\t\tship_strategy : FORWARD\n\n\t\tStage 3 : Operator\n\t\t\tcontent : Calc(select=[ip AS namenode, 0:BIGINT AS node_id, (data SUBSTRING ((_UTF-16LE'completeFile' POSITION data) + 14) SUBSTRING (((_UTF-16LE' is closed ' POSITION data) - (_UTF-16LE'completeFile' POSITION data)) - 14)) AS path, CAST(CAST(_UTF-16LE'completeFile')) AS event_type, ts], where=[((data LIKE _UTF-16LE'%completeFile%') AND ((_UTF-16LE'/.' POSITION (data SUBSTRING ((_UTF-16LE'completeFile' POSITION data) + 14) SUBSTRING (((_UTF-16LE' is closed ' POSITION data) - (_UTF-16LE'completeFile' POSITION data)) - 14))) = 0))])\n\t\t\tship_strategy : FORWARD\n\n\t\t\tStage 5 : Operator\n\t\t\t\tcontent : Calc(select=[ip AS namenode, 0:BIGINT AS node_id, (data SUBSTRING ((_UTF-16LE'cmd=rename' POSITION data) + 10) SUBSTRING (((_UTF-16LE'perm=' POSITION data) - (_UTF-16LE'cmd=rename' POSITION data)) - 10)) AS path, CAST(CAST(_UTF-16LE'renameFile')) AS event_type, ts], where=[(data LIKE _UTF-16LE'%cmd=rename%')])\n\t\t\t\tship_strategy : FORWARD\n\n\t\t\t\tStage 7 : Operator\n\t\t\t\t\tcontent : Calc(select=[ip AS namenode, 0:BIGINT AS node_id, (data SUBSTRING ((_UTF-16LE'cmd=delete' POSITION data) + 10) SUBSTRING (((_UTF-16LE'perm=' POSITION data) - (_UTF-16LE'cmd=delete' POSITION data)) - 10)) AS path, CAST(CAST(_UTF-16LE'deleteFile')) AS event_type, ts], where=[((data LIKE _UTF-16LE'%cmd=delete%') AND ((_UTF-16LE'/.' POSITION (data SUBSTRING ((_UTF-16LE'cmd=delete' POSITION data) + 10) SUBSTRING (((_UTF-16LE'perm=' POSITION data) - (_UTF-16LE'cmd=delete' POSITION data)) - 10))) = 0))])\n\t\t\t\t\tship_strategy : FORWARD\n\n\t\t\t\t\tStage 4 : Data Sink\n\t\t\t\t\t\tcontent : Sink: Sink(table=[default_catalog.default_database.hdfs_namenode_nodeevent], fields=[namenode, node_id, path, event_type, ts])\n\t\t\t\t\t\tship_strategy : FORWARD\n\n\t\t\t\t\t\tStage 6 : Data Sink\n\t\t\t\t\t\t\tcontent : Sink: Sink(table=[default_catalog.default_database.hdfs_namenode_nodeevent], fields=[namenode, node_id, path, event_type, ts])\n\t\t\t\t\t\t\tship_strategy : FORWARD\n\n\t\t\t\t\t\t\tStage 8 : Data Sink\n\t\t\t\t\t\t\t\tcontent : Sink: Sink(table=[default_catalog.default_database.hdfs_namenode_nodeevent], fields=[namenode, node_id, path, event_type, ts])\n\t\t\t\t\t\t\t\tship_strategy : FORWARD\n```\n\n![image-20211227115029562](../../images/flink/optimize-1.png)\n\n\n\n可以看到Stage 2后的ship_strategy是FORWARD，并且flink会所有算子都使用默认的并行度，就会将Stage 1和2 chain在一起，而后续都是FORWARD的数据shuffle模式, 所以所有的算子都是chain在一起的，为了更详细的观察每个算子的数据情况，设置`set pipeline.operator-chaining=false;`\n\njobGraph变成：\n\n![image-20211227115540710](../../images/flink/optimoze-2.png)\n\n![image-20211227115628817](../../images/flink/optimize-3.png)\n\n对于每个算子而言，能利用的最大并行度就等于Date Source的topic的分区数。这个就限制了程序后续处理的吞吐。\n\n## 优化后\n\n自定义并行度，通过在source table的语句里添加source.parallelism参数设置source算子的并行度，调大作业的默认并行度即可。\n\n```properties\nsailfish扩展的set语法，将source.parallelism添加到r_dataplat.ods_hadoop_nn_log流表的ddl中\n\nset table.r_dataplat.ods_hadoop_nn_log.source.parallelism=6;\n```\n\n```\n== Physical Execution Plan ==\nStage 1 : Data Source\n\tcontent : Source: TableSourceScan(table=[[default_catalog, default_database, ods_hadoop_nn_log]], fields=[app, module, type, data, ip, logFile, timestamp])\n\n\tStage 2 : Operator\n\t\tcontent : Calc(select=[ip, timestamp AS ts, data], where=[((CAST(module) = _UTF-16LE'hdfs':VARCHAR(4) CHARACTER SET \"UTF-16LE\") AND (CAST(type) = _UTF-16LE'nn':VARCHAR(2) CHARACTER SET \"UTF-16LE\") AND ((data LIKE _UTF-16LE'%completeFile%') OR (data LIKE _UTF-16LE'%cmd=rename%') OR (data LIKE _UTF-16LE'%cmd=delete%')))])\n\t\tship_strategy : REBALANCE\n\n\t\tStage 3 : Operator\n\t\t\tcontent : Calc(select=[ip AS namenode, 0:BIGINT AS node_id, (data SUBSTRING ((_UTF-16LE'completeFile' POSITION data) + 14) SUBSTRING (((_UTF-16LE' is closed ' POSITION data) - (_UTF-16LE'completeFile' POSITION data)) - 14)) AS path, CAST(CAST(_UTF-16LE'completeFile')) AS event_type, ts], where=[((data LIKE _UTF-16LE'%completeFile%') AND ((_UTF-16LE'/.' POSITION (data SUBSTRING ((_UTF-16LE'completeFile' POSITION data) + 14) SUBSTRING (((_UTF-16LE' is closed ' POSITION data) - (_UTF-16LE'completeFile' POSITION data)) - 14))) = 0))])\n\t\t\tship_strategy : FORWARD\n\n\t\t\tStage 5 : Operator\n\t\t\t\tcontent : Calc(select=[ip AS namenode, 0:BIGINT AS node_id, (data SUBSTRING ((_UTF-16LE'cmd=rename' POSITION data) + 10) SUBSTRING (((_UTF-16LE'perm=' POSITION data) - (_UTF-16LE'cmd=rename' POSITION data)) - 10)) AS path, CAST(CAST(_UTF-16LE'renameFile')) AS event_type, ts], where=[(data LIKE _UTF-16LE'%cmd=rename%')])\n\t\t\t\tship_strategy : FORWARD\n\n\t\t\t\tStage 7 : Operator\n\t\t\t\t\tcontent : Calc(select=[ip AS namenode, 0:BIGINT AS node_id, (data SUBSTRING ((_UTF-16LE'cmd=delete' POSITION data) + 10) SUBSTRING (((_UTF-16LE'perm=' POSITION data) - (_UTF-16LE'cmd=delete' POSITION data)) - 10)) AS path, CAST(CAST(_UTF-16LE'deleteFile')) AS event_type, ts], where=[((data LIKE _UTF-16LE'%cmd=delete%') AND ((_UTF-16LE'/.' POSITION (data SUBSTRING ((_UTF-16LE'cmd=delete' POSITION data) + 10) SUBSTRING (((_UTF-16LE'perm=' POSITION data) - (_UTF-16LE'cmd=delete' POSITION data)) - 10))) = 0))])\n\t\t\t\t\tship_strategy : FORWARD\n\n\t\t\t\t\tStage 4 : Data Sink\n\t\t\t\t\t\tcontent : Sink: Sink(table=[default_catalog.default_database.hdfs_namenode_nodeevent], fields=[namenode, node_id, path, event_type, ts])\n\t\t\t\t\t\tship_strategy : FORWARD\n\n\t\t\t\t\t\tStage 6 : Data Sink\n\t\t\t\t\t\t\tcontent : Sink: Sink(table=[default_catalog.default_database.hdfs_namenode_nodeevent], fields=[namenode, node_id, path, event_type, ts])\n\t\t\t\t\t\t\tship_strategy : FORWARD\n\n\t\t\t\t\t\t\tStage 8 : Data Sink\n\t\t\t\t\t\t\t\tcontent : Sink: Sink(table=[default_catalog.default_database.hdfs_namenode_nodeevent], fields=[namenode, node_id, path, event_type, ts])\n\t\t\t\t\t\t\t\tship_strategy : FORWARD\n```\n\n![image-20211227120322329](../../images/flink/optimize-4.png)\n\n具体实现可以参照\n[PR](https://github.com/apache/flink/pull/18277/files)","tags":["flink"],"categories":["flink"]},{"title":"clickhouse+JuiceFS冷热数据分层+读写分离的方案","url":"/clickhouse/juicefs/","content":"\n# clickhouse+JuiceFS冷热数据分层+读写分离的方案\n\n## JuiceFS简介\n\njuiceFS的介绍可以参考[juiceFS github](https://github.com/juicedata/juicefs)\n\nJuiceFS 是在 GNU Affero General Public License v3.0 下发布的高性能 POSIX 文件系统。它专门针对云原生环境进行了优化。使用 JuiceFS 存储数据，数据本身会持久化到对象存储（如 S3、oss）中，数据对应的元数据可以根据需要持久化到 Redis、MySQL、SQLite 等各种数据库引擎中。\n\nJuiceFS 可以简单方便地将海量云存储直接连接到大数据、机器学习、人工智能以及已经投入生产环境的各种应用平台，无需修改代码，您可以像使用本地存储一样高效地使用海量云存储。\n\n<!--more-->\n\n## 为啥会考虑使用JuiceFS\n\n与Hadoop 生态组件通常依赖 HDFS 作为底层的数据存储不同，ClickHouse 使用本地盘来自己管理数据，[社区建议](https://clickhouse.tech/docs/en/operations/tips/#storage-subsystem)使用 SSD +raid盘的方式作为存储介质来提升性能。但受限于本地盘的容量上限以及 SSD 盘的价格，用户很难在容量、成本和性能这三者之间找到一个好的平衡。由于JuiceFS 是基于对象存储实现并完全兼容 POSIX 的开源分布式文件系统，同时 JuiceFS 的数据缓存特性可以智能管理查询热点数据，非常适合作为 ClickHouse 的存储系统，下面将详细介绍这个方案。\n\n## 方案细节\n\n由于JuiceFS完全兼容POSIX协议，因此可以将JuiceFS mount到本地磁盘，不同的节点可以都使用同一个JuiceFS进行mount，并使用不同的目录进行区别。\n\n为了演示，使用hdfs作为JuiceFS的存储，使用redis作为JuiceFS的元数据存储。\n\n### 初始化JuiceFS\n\n```shell\njuicefs  format --storage hdfs \\\n--bucket nn1.local.hadoop3.hy:8020,nn2.local.hadoop3.hy:8020 \\\n--access-key <user>@<token> \\\nredis://:<pwd>@<ip>:<port>/<db>  jfsck\n```\n\n### mount JuiceFS\n\n```shell\nsudo juicefs mount --cache-dir /data5/jfsCache -d redis://:<pwd>@<ip>:<port>/<db> /mnt/jfshdfs\n```\n\n### 配置clickhouse磁盘和存储策略\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<clickhouse>\n    <storage_configuration>\n        <disks>            \n            <default>\n             <keep_free_space_bytes>1048576000</keep_free_space_bytes>\n            </default>\n            <data2>\n                <path>/data2/clickhouse/data/</path>\n                <keep_free_space_bytes>1048576000</keep_free_space_bytes>\n            </data2>   \n            <hdfs>\n                <path>/mnt/jfshdfs/68/</path>\n            </hdfs>\n        </disks>\n        <policies>\n            <default>\n                <volumes>\n                    <cold>\n                        <disk>default</disk>\n                        <disk>data2</disk>\n                    </cold>            \n                </volumes>\n                <move_factor>0.2</move_factor>\n            </default>\n            <cold>\n                <volumes>\n                    <backup>\n                        <disk>hdfs</disk>\n                    </backup>\n                </volumes>\n            </cold>\n            <hot_cold>\n                <volumes>\n                    <hot>\n                        <disk>default</disk>\n                        <disk>data2</disk>\n                        <max_data_part_size_bytes>104857600</max_data_part_size_bytes>\n                    </hot>\n                    <cold>\n                        <disk>hdfs</disk>\n                    </cold>                                    \n                </volumes>\n                <move_factor>0.2</move_factor>\n            </hot_cold>\n          </policies>\n    </storage_configuration>\n</clickhouse>\n```\n\n配置中定义了`hot_cold`策略，该策略中包含`hot`和`cold`两个磁盘卷，`hot`中指定的是两个本地磁盘，`cold`中指定的是JuiceFS远程共享文件系统，`max_data_part_size_bytes`表示可以存储在卷的任何磁盘上的部分的最大大小。如果合并部分的大小估计大于 max_data_part_size_bytes，则该部分将被写入下一个卷。基本上，此功能允许将新/小部件保留在热 (SSD) 卷上，并在它们达到大尺寸时将它们移至冷 (HDD) 卷。为了测试这里设置为100M`move_factor`配置表示当磁盘的容量超过 80% 时触发数据移动到 JuiceFS。\n\n重启clickhouse-server，查看存储策略是否生效\n\n![image-20220103164803141](../../images/clickhouse/juicefs-4.png)\n\n### 建表测试\n\n#### 建表\n\n新建一张按天分区的表，默认的存储策略设置为`hot_cold`, 设置超过20天的数据`move`到`cold`磁盘卷中\n\n```\nCREATE TABLE default.t_report_yspwl_dm_transm_mk_info2_day\n(\n    `day` Date DEFAULT toDate(its),\n    `its` UInt32,\n    `appid` Int32,\n    `line` Int32,\n    `user_isp` LowCardinality(String),\n    `user_province` LowCardinality(String),\n    `cdnip` LowCardinality(String),\n    `networktype` LowCardinality(String),\n    `game_name` LowCardinality(String),\n    `p2p` Int32,\n    `p2p_origin_type` LowCardinality(String),\n    `harddecode` Int32,\n    `bad_cnt` Int64,\n    `log_cnt` Int64,\n    `tot_log_cnt` Int64,\n    `repeat_times` Int64,\n    `_cnt` UInt32 DEFAULT CAST(1, 'UInt32'),\n    `peak_flag` Int32\n)\nENGINE = MergeTree\nPARTITION BY toYYYYMMDD(day)\nORDER BY its\nTTL day + toIntervalDay(20) TO VOLUME 'cold'\nSETTINGS index_granularity = 8192, storage_policy = 'hot_cold'\n```\n\n#### 插入数据\n\n1. 当插入20天前的数据，parts会直接写入到`cold`中\n2. 当插入`2021-12-17`号的数据，查看parts分布\n\n![image-20220103153506496](../../images/clickhouse/juicefs-1.png)\n\n过一会等后台线程merge后再查看,可以看到17号的数据大于100MB的parts被存进了cold存储中\n\n![image-20220103154047920](../../images/clickhouse/juicefs-2.png)\n\n查看目录文件\n\n![image-20220103154700397](../../images/clickhouse/juicefs-3.png)\n\n## 一种读写分离的思路\n\n采用JuiceFS存储clickhouse冷数据，可以有效的扩展clickhouse单节点磁盘不足的问题，同时由于JuiceFS的共享特性，可以引申出一种读写分离的架构，如下图所示。对于ck本地表`default.table1`，在离线场景下，可以从k8s上启动临时的ck server，将离线数据写入到对应表中，当分区的数据写完后，可以detach partition，然后将这个partition对应的目录move到正式集群对应的节点对应的目录中去，再在正式集群中attach进行使用。此做法在重跑大量历史数据时，可以大大减轻正式集群的节点写入压力。\n\n<img src=\"../../images/clickhouse/juicefs-5.png\" alt=\"企业微信截图_7f1369c4-b3e7-46c2-9bbd-87219e5dd8fe\" style=\"zoom:30%;\" />\n\n## 参考\n\n[[ClickHouse 存算分离架构探索](https://juicefs.com/blog/cn/posts/clickhouse-disaggregated-storage-and-compute-practice/)](https://juicefs.com/blog/cn/posts/clickhouse-disaggregated-storage-and-compute-practice/)\n\n[Shopee ClickHouse 冷热数据分离存储架构与实践](https://mp.weixin.qq.com/s/eELcw0v2U9UXaHnfADj3tQ)\n\n","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"clickhouse源码构建","url":"/clickhouse/build/","content":"\n# clickhouse源码构建\n\n## 概述\n- 官方文档：https://clickhouse.com/docs/en/development/build/\n- 源码分支 21.12\n- 环境依赖\n  - ubuntu: 16.04\n  - cmake: 3.20.6\n  - clang: clang-13\n\n<!-- more -->\n\n## 步骤\n\n- 准备源码: \n\n  clickhouse官方库为：https://github.com/ClickHouse/ClickHouse\n\n  `git clone -b 21.12 https://github.com/ClickHouse/ClickHouse.git`，可切换到其他具体的分支\n\n- 执行git submodule更新相关的依赖包，依赖包会下载到``ClickHouse/contrib`下\n\n  ```shell\n  git submodule sync\n  # 初始化并下载相关依赖\n  git submodule update --init --recursive \n  ```\n\n- 编译环境下载相关的依赖，如cmake，clang等\n\n- 将下面脚本`build.sh`放在ClickhouseHouse目录下，执行`bash build.sh`\n\n  ```\n  mkdir build-clang\n  cd build-clang\n  \n  je=1\n  tc=1\n  BT=Release\n  #BT=Debug\n  \n  export TMPDIR=/data/tmp\n  /opt/cmake-3.20.6-linux-x86_64/bin/cmake .. -DENABLE_JEMALLOC=${je}  -DENABLE_TCMALLOC=${tc} -DCMAKE_EXPORT_COMPILE_COMMANDS=1  -DENABLE_TESTS=OFF  -DCMAKE_BUILD_TYPE=${BT} -DENABLE_RDKAFKA=0 -DCMAKE_CXX_COMPILER=`which clang++-13` -DCMAKE_C_COMPILER=`which clang-13`\n  \n  if [ $? -ne 0 ]; then\n    echo \"cmake faild\"\n    exit 1\n  fi\n  \n  \n  ninja -j20\n  \n  \n  if [ $? -ne 0 ]; then\n    echo \"faild\"\n    exit 1\n  fi\n  ```\n\n- 编译完可执行文件在`build-clang/programs下`\n\n  ```\n  $ tree -L 1 programs\n  programs\n  ├── bash-completion\n  ├── benchmark\n  ├── clickhouse\n  ├── clickhouse-benchmark -> clickhouse\n  ├── clickhouse-client -> clickhouse\n  ├── clickhouse-compressor -> clickhouse\n  ├── clickhouse-copier -> clickhouse\n  ├── clickhouse-extract-from-config -> clickhouse\n  ├── clickhouse-format -> clickhouse\n  ├── clickhouse-git-import -> clickhouse\n  ├── clickhouse-keeper -> clickhouse\n  ├── clickhouse-keeper-converter -> clickhouse\n  ├── clickhouse-library-bridge\n  ├── clickhouse-local -> clickhouse\n  ├── clickhouse-obfuscator -> clickhouse\n  ├── clickhouse-odbc-bridge\n  ├── clickhouse-server -> clickhouse\n  ├── clickhouse-static-files-disk-uploader -> clickhouse\n  ├── client\n  ├── CMakeFiles\n  ├── cmake_install.cmake\n  ├── compressor\n  ├── copier\n  ├── CTestTestfile.cmake\n  ├── extract-from-config\n  ├── format\n  ├── git-import\n  ├── install\n  ├── keeper\n  ├── keeper-converter\n  ├── library-bridge\n  ├── local\n  ├── obfuscator\n  ├── odbc-bridge\n  ├── server\n  └── static-files-disk-uploader\n  ```\n\n## 碰到的问题\n\n各种包依赖，这个时候看报错日志，把相关依赖解决就行","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"ck集群运维注意事项","url":"/clickhouse/dev_notify/","content":"\n# ck集群运维注意事项\n\n<!-- more -->\n\n1. alter时，只需要在一个分片上执行即可\n2. 遇到zookeeper session expire删除不了表，可以先detach再attach\n3. ck mutation可以保证异步操作不会删除新写入的数据，可以通过system.mutations查看进度\n4. 多磁盘空出两块盘，单独部署zk,并且将zk的log和snapshot分开\n5. 遇到too many parts时，需要控制写入速度。sinker看看是不是负载没有好,可以 alter table t_dw_huya_page_pv_ck on cluster single modify setting parts_to_throw_insert=600 增大抛异常的值，但是本质上还是后台merge跟不上插入，可以增大io性能，或减少分区\n6. 没有副本的cluster,要设置 <internal_replication>false</internal_replication>\n7. 修改```<background_pool_size>64</background_pool_size> \n      <background_schedule_pool_size>64</background_schedule_pool_size>\n      <background_move_pool_size>32</background_move_pool_size>```调大后台执行线程数\n8. 修改```<max_partitions_per_insert_block>1000</max_partitions_per_insert_block>``` 解决 DB::Exception: Too many partitions for single INSERT block (more than 100)的问题\n9. 修改```<max_replicated_merges_in_queue>200</max_replicated_merges_in_queue>\n        <max_replicated_mutations_in_queue>200</max_replicated_mutations_in_queue>```\n解决  Number of queued merges (36) and part mutations (0) is greater than max_replicated_merges_in_queue (16)  的错误\n11. 查看集群事件和指标\nSELECT \n    event_time,\n    ProfileEvent_InsertQuery,\n    runningDifference(ProfileEvent_InsertQuery) ins_per_sec\nFROM system.metric_log\nWHERE event_date = today()\nORDER BY event_time DESC\nLIMIT 50;\nSELECT value\nFROM system.events\nWHERE event LIKE 'InsertQuery'\n\n12. 查看多少mutation没执行```select table,command,mutation_id,create_time,is_done,latest_fail_time,latest_fail_reason,parts_to_do from system.mutations where table= 't_oexp_precomputation_all_metric' order by create_time limit  10```\n13. 碰到 The local set of parts of table default.dwd_test doesn’t look like the set of parts in ZooKeeper 的问题，一般都是手动操作了表的ddl导致不同步，可以把metadata下的这个表先移除，让server可以正常启动，然后再重建表，数据会自动同步\n14.  select count() as c ,database||'.'||table from system.replication_queue where type='MUTATE_PART'  group by database,table order by c\n15.  select count() as c ,database||'.'||table from system.mutations where is_done=0 group by database,table order by c\n16.  遇到卡住的时候，可以先把卡住的表detach，然后批次attach回来","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"clickhouse集群2副本安装","url":"/clickhouse/install/","content":"\n\n\n# clickhouse集群2副本安装\n\n## 使用4台机器\n\n```\n节点1  10.64.148.133\n节点2  10.64.148.134\n节点3  10.64.148.135\n节点4  10.64.138.24\n```\n<!-- more -->\n## 配置路径\n\n```\nroot@ip-10-64-138-24:/data/clickhouse# tree\n.\n├── bin\n│   ├── clickhouse\n│   ├── clickhouse-client -> /data/clickhouse/bin/clickhouse\n│   └── clickhouse-server -> /data/clickhouse/bin/clickhouse\n├── conf\n│   ├── config.d\n│   │   ├── config.base.xml\n│   │   ├── config.disks.xml\n│   │   ├── config.macros.xml\n│   │   └── config.remote_servers.xml\n│   ├── config.xml\n│   ├── env.ini\n│   ├── users.d\n│   │   └── users.huya.xml\n│   └── users.xml\n├── format_schemas\n├── log\n│   ├── clickhouse-server.err.log\n│   ├── clickhouse-server.log\n│   ├── stderr.log\n│   └── stdout.log\n├── start_ch.sh\n├── tmp\n└── user_files\n```\n\n- `/data/clickhouse`为安装目录\n\n- `bin/`下为自行编译的clickhouse二进制文件\n\n- `conf/`下为ck的配置, `config.xml`为启动进程时默认加载的，\n  - `config.d/*.xml`为具体的子配置，具体包括\n    - `config.base.xml`: 基础配置\n    - `config.disks.xml`: 磁盘和存储策略配置\n    - `config.macros.xml`:宏定义配置\n    - `config.remote_servers.xml`: 集群配置\n  - `env.ini`： 环境配置\n- `users.d`: 存储用户配置，访问权限之类\n- `start_ch.sh`:启动脚本\n\n对应每个文件如下：\n\n### 默认启动配置 /data/clickhouse/conf/config.xml\n\n```\n<?xml version=\"1.0\"?>\n<!--\n  NOTE: User and query level settings are set up in \"users.xml\" file.\n-->\n<yandex>\n    <logger>\n        <!-- Possible levels: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105 -->\n        <level>trace</level>\n        <log>/data/clickhouse/log/clickhouse-server.log</log>\n        <errorlog>/data/clickhouse/log/clickhouse-server.err.log</errorlog>\n        <size>1000M</size>\n        <count>10</count>\n        <!-- <console>1</console> --> <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->\n    </logger>\n    <!--display_name>production</display_name--> <!-- It is the name that will be shown in the client -->\n    <http_port>8123</http_port>\n    <tcp_port>9000</tcp_port>\n    <mysql_port>9004</mysql_port>\n    <!-- For HTTPS and SSL over native protocol. -->\n    <!--\n    <https_port>8443</https_port>\n    <tcp_port_secure>9440</tcp_port_secure>\n    -->\n\n    <!-- Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71 -->\n    <openSSL>\n        <server> <!-- Used for https server AND secure tcp port -->\n            <!-- openssl req -subj \"/CN=localhost\" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt -->\n            <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>\n            <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile>\n            <!-- openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096 -->\n            <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>\n            <verificationMode>none</verificationMode>\n            <loadDefaultCAFile>true</loadDefaultCAFile>\n            <cacheSessions>true</cacheSessions>\n            <disableProtocols>sslv2,sslv3</disableProtocols>\n            <preferServerCiphers>true</preferServerCiphers>\n        </server>\n\n        <client> <!-- Used for connecting to https dictionary source -->\n            <loadDefaultCAFile>true</loadDefaultCAFile>\n            <cacheSessions>true</cacheSessions>\n            <disableProtocols>sslv2,sslv3</disableProtocols>\n            <preferServerCiphers>true</preferServerCiphers>\n            <!-- Use for self-signed: <verificationMode>none</verificationMode> -->\n            <invalidCertificateHandler>\n                <!-- Use for self-signed: <name>AcceptCertificateHandler</name> -->\n                <name>RejectCertificateHandler</name>\n            </invalidCertificateHandler>\n        </client>\n    </openSSL>\n\n    <!-- Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123 -->\n    <!--\n    <http_server_default_response><![CDATA[<html ng-app=\"SMI2\"><head><base href=\"http://ui.tabix.io/\"></head><body><div ui-view=\"\" class=\"content-ui\"></div><script src=\"http://loader.tabix.io/master.js\"></script></body></html>]]></http_server_default_response>\n    -->\n\n    <!-- Port for communication between replicas. Used for data exchange. -->\n    <interserver_http_port>9009</interserver_http_port>\n\n    <!-- Hostname that is used by other replicas to request this server.\n         If not specified, than it is determined analoguous to 'hostname -f' command.\n         This setting could be used to switch replication to another network interface.\n      -->\n    <!--\n    <interserver_http_host>example.yandex.ru</interserver_http_host>\n    -->\n\n    <!-- Listen specified host. use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere. -->\n    <!-- <listen_host>::</listen_host> -->\n    <!-- Same for hosts with disabled ipv6: -->\n    <!-- <listen_host>0.0.0.0</listen_host> -->\n\n    <!-- Default values - try listen localhost on ipv4 and ipv6: -->\n\n    <listen_host>0.0.0.0</listen_host>\n    <!-- <listen_host>127.0.0.1</listen_host> -->\n\n    <!-- Don't exit if ipv6 or ipv4 unavailable, but listen_host with this protocol specified -->\n    <!-- <listen_try>0</listen_try> -->\n\n    <!-- Allow listen on same address:port -->\n    <!-- <listen_reuse_port>0</listen_reuse_port> -->\n\n    <!-- <listen_backlog>64</listen_backlog> -->\n\n    <max_connections>4096</max_connections>\n    <keep_alive_timeout>3</keep_alive_timeout>\n\n    <!-- Maximum number of concurrent queries. -->\n    <max_concurrent_queries>500</max_concurrent_queries>\n\n    <!-- Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve\n         correct maximum value. -->\n    <!-- <max_open_files>262144</max_open_files> -->\n\n    <!-- Size of cache of uncompressed blocks of data, used in tables of MergeTree family.\n         In bytes. Cache is single for server. Memory is allocated only on demand.\n         Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).\n         Uncompressed cache is advantageous only for very short queries and in rare cases.\n      -->\n    <uncompressed_cache_size>8589934592</uncompressed_cache_size>\n\n    <!-- Approximate size of mark cache, used in tables of MergeTree family.\n         In bytes. Cache is single for server. Memory is allocated only on demand.\n         You should not lower this value.\n      -->\n    <mark_cache_size>5368709120</mark_cache_size>\n\n\n    <!-- Path to data directory, with trailing slash. -->\n    <path>/data/clickhouse/</path>\n\n    <!-- Path to temporary data for processing hard queries. -->\n    <tmp_path>/data/clickhouse/tmp/</tmp_path>\n\n    <!-- Policy from the <storage_configuration> for the temporary files.\n         If not set <tmp_path> is used, otherwise <tmp_path> is ignored.\n\n         Notes:\n         - move_factor              is ignored\n         - keep_free_space_bytes    is ignored\n         - max_data_part_size_bytes is ignored\n         - you must have exactly one volume in that policy\n    -->\n    <!-- <tmp_policy>tmp</tmp_policy> -->\n\n    <!-- Directory with user provided files that are accessible by 'file' table function. -->\n    <user_files_path>/data/clickhouse/user_files/</user_files_path>\n\n    <!-- Path to configuration file with users, access rights, profiles of settings, quotas. -->\n    <users_config>users.xml</users_config>\n\n    <!-- Default profile of settings. -->\n    <default_profile>default</default_profile>\n\n    <!-- System profile of settings. This settings are used by internal processes (Buffer storage, Distibuted DDL worker and so on). -->\n    <!-- <system_profile>default</system_profile> -->\n\n    <!-- Default database. -->\n    <default_database>default</default_database>\n\n    <!-- Server time zone could be set here.\n\n         Time zone is used when converting between String and DateTime types,\n          when printing DateTime in text formats and parsing DateTime from text,\n          it is used in date and time related functions, if specific time zone was not passed as an argument.\n\n         Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.\n         If not specified, system time zone at server startup is used.\n\n         Please note, that server could display time zone alias instead of specified name.\n         Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC.\n    -->\n    <timezone>Asia/Shanghai</timezone>\n\n    <!-- You can specify umask here (see \"man umask\"). Server will apply it on startup.\n         Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).\n    -->\n    <!-- <umask>022</umask> -->\n\n    <!-- Perform mlockall after startup to lower first queries latency\n          and to prevent clickhouse executable from being paged out under high IO load.\n         Enabling this option is recommended but will lead to increased startup time for up to a few seconds.\n    -->\n    <mlock_executable>false</mlock_executable>\n    <!-- Configuration of clusters that could be used in Distributed tables.\n         https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n      -->\n    <remote_servers incl=\"clickhouse_remote_servers\" >\n        <!-- Test only shard config for testing distributed storage -->\n    </remote_servers>\n\n    <!-- The list of hosts allowed to use in URL-related storage engines and table functions.\n        If this section is not present in configuration, all hosts are allowed.\n    -->\n    <remote_url_allow_hosts>\n        <!-- Host should be specified exactly as in URL. The name is checked before DNS resolution.\n            Example: \"yandex.ru\", \"yandex.ru.\" and \"www.yandex.ru\" are different hosts.\n                    If port is explicitly specified in URL, the host:port is checked as a whole.\n                    If host specified here without port, any port with this host allowed.\n                    \"yandex.ru\" -> \"yandex.ru:443\", \"yandex.ru:80\" etc. is allowed, but \"yandex.ru:80\" -> only \"yandex.ru:80\" is allowed.\n            If the host is specified as IP address, it is checked as specified in URL. Example: \"[2a02:6b8:a::a]\".\n            If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.\n        -->\n\n        <!-- Regular expression can be specified. RE2 engine is used for regexps.\n            Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter\n            (forgetting to do so is a common source of error).\n        -->\n    </remote_url_allow_hosts>\n\n    <!-- If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.\n         By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.\n         Values for substitutions are specified in /yandex/name_of_substitution elements in that file.\n      -->\n\n    <!-- ZooKeeper is used to store metadata about replicas, when using Replicated tables.\n         Optional. If you don't use replicated tables, you could omit that.\n\n         See https://clickhouse.yandex/docs/en/table_engines/replication/\n      -->\n\n    <zookeeper incl=\"zookeeper-servers\" optional=\"true\"/>\n\n    <!-- Substitutions for parameters of replicated tables.\n          Optional. If you don't use replicated tables, you could omit that.\n\n         See https://clickhouse.yandex/docs/en/table_engines/replication/#creating-replicated-tables\n      -->\n    <macros incl=\"macros\" optional=\"true\" />\n\n\n    <!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. -->\n    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>\n\n\n    <!-- Maximum session timeout, in seconds. Default: 3600. -->\n    <max_session_timeout>3600</max_session_timeout>\n\n    <!-- Default session timeout, in seconds. Default: 60. -->\n    <default_session_timeout>60</default_session_timeout>\n\n    <!-- Sending data to Graphite for monitoring. Several sections can be defined. -->\n    <!--\n        interval - send every X second\n        root_path - prefix for keys\n        hostname_in_path - append hostname to root_path (default = true)\n        metrics - send data from table system.metrics\n        events - send data from table system.events\n        asynchronous_metrics - send data from table system.asynchronous_metrics\n    -->\n    <!--\n    <graphite>\n        <host>localhost</host>\n        <port>42000</port>\n        <timeout>0.1</timeout>\n        <interval>60</interval>\n        <root_path>one_min</root_path>\n        <hostname_in_path>true</hostname_in_path>\n\n        <metrics>true</metrics>\n        <events>true</events>\n        <events_cumulative>false</events_cumulative>\n        <asynchronous_metrics>true</asynchronous_metrics>\n    </graphite>\n    <graphite>\n        <host>localhost</host>\n        <port>42000</port>\n        <timeout>0.1</timeout>\n        <interval>1</interval>\n        <root_path>one_sec</root_path>\n\n        <metrics>true</metrics>\n        <events>true</events>\n        <events_cumulative>false</events_cumulative>\n        <asynchronous_metrics>false</asynchronous_metrics>\n    </graphite>\n    -->\n\n    <!-- Serve endpoint fot Prometheus monitoring. -->\n    <!--\n        endpoint - mertics path (relative to root, statring with \"/\")\n        port - port to setup server. If not defined or 0 than http_port used\n        metrics - send data from table system.metrics\n        events - send data from table system.events\n        asynchronous_metrics - send data from table system.asynchronous_metrics\n    -->\n    <!--\n    <prometheus>\n        <endpoint>/metrics</endpoint>\n        <port>9363</port>\n\n        <metrics>true</metrics>\n        <events>true</events>\n        <asynchronous_metrics>true</asynchronous_metrics>\n    </prometheus>\n    -->\n\n    <!-- Query log. Used only for queries with setting log_queries = 1. -->\n    <query_log>\n        <!-- What table to insert data. If table is not exist, it will be created.\n             When query log structure is changed after system update,\n              then old table will be renamed and new table will be created automatically.\n        -->\n        <database>system</database>\n        <table>query_log</table>\n        <!--\n            PARTITION BY expr https://clickhouse.yandex/docs/en/table_engines/custom_partitioning_key/\n            Example:\n                event_date\n                toMonday(event_date)\n                toYYYYMM(event_date)\n                toStartOfHour(event_time)\n        -->\n        <partition_by>toYYYYMMDD(event_date)</partition_by>\n\n        <!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,\n             Example: <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024</engine>\n          -->\n\n        <!-- Interval of flushing data. -->\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </query_log>\n\n    <!-- Trace log. Stores stack traces collected by query profilers.\n         See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->\n    <trace_log>\n        <database>system</database>\n        <table>trace_log</table>\n\n        <partition_by>toYYYYMMDD(event_date)</partition_by>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </trace_log>\n\n    <!-- Query thread log. Has information about all threads participated in query execution.\n         Used only for queries with setting log_query_threads = 1. -->\n    <query_thread_log>\n        <database>system</database>\n        <table>query_thread_log</table>\n        <partition_by>toYYYYMM(event_date)</partition_by>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </query_thread_log>\n\n    <!-- Uncomment if use part log.\n         Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).\n         -->\n    <part_log>\n        <database>system</database>\n        <table>part_log</table>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </part_log>\n\n\n    <!-- Uncomment to write text log into table.\n         Text log contains all information from usual server log but stores it in structured and efficient way.\n         The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.\n           -->\n    <text_log>\n        <database>system</database>\n        <table>text_log</table>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n        <level>trace</level>\n    </text_log>\n\n\n    <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with \"collect_interval_milliseconds\" interval. -->\n    <metric_log>\n        <database>system</database>\n        <table>metric_log</table>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n        <collect_interval_milliseconds>1000</collect_interval_milliseconds>\n    </metric_log>\n\n    <!-- Parameters for embedded dictionaries, used in Yandex.Metrica.\n         See https://clickhouse.yandex/docs/en/dicts/internal_dicts/\n    -->\n\n    <!-- Path to file with region hierarchy. -->\n    <!-- <path_to_regions_hierarchy_file>/opt/geo/regions_hierarchy.txt</path_to_regions_hierarchy_file> -->\n\n    <!-- Path to directory with files containing names of regions -->\n    <!-- <path_to_regions_names_files>/opt/geo/</path_to_regions_names_files> -->\n\n\n    <!-- Configuration of external dictionaries. See:\n         https://clickhouse.yandex/docs/en/dicts/external_dicts/\n    -->\n    <dictionaries_config>*_dictionary.xml</dictionaries_config>\n\n    <!-- Uncomment if you want data to be compressed 30-100% better.\n         Don't do that if you just started using ClickHouse.\n      -->\n    <compression incl=\"clickhouse_compression\">\n    <!--\n        <!- - Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used. - ->\n        <case>\n\n            <!- - Conditions. All must be satisfied. Some conditions may be omitted. - ->\n            <min_part_size>10000000000</min_part_size>        <!- - Min part size in bytes. - ->\n            <min_part_size_ratio>0.01</min_part_size_ratio>   <!- - Min size of part relative to whole table size. - ->\n\n            <!- - What compression method to use. - ->\n            <method>zstd</method>\n        </case>\n    -->\n    </compression>\n\n    <!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.\n         Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. -->\n    <distributed_ddl>\n        <!-- Path in ZooKeeper to queue with DDL queries -->\n        <path>/clickhouse/task_queue/ddl</path>\n\n        <!-- Settings from this profile will be used to execute DDL queries -->\n        <!-- <profile>default</profile> -->\n    </distributed_ddl>\n\n    <!-- Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h -->\n    <!--\n    <merge_tree>\n        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>\n    </merge_tree>\n    -->\n\n    <!-- Protection from accidental DROP.\n         If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.\n         If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.\n         By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.\n         The same for max_partition_size_to_drop.\n         Uncomment to disable protection.\n    -->\n    <max_table_size_to_drop>0</max_table_size_to_drop>\n    <max_partition_size_to_drop>0</max_partition_size_to_drop>\n\n    <!-- Example of parameters for GraphiteMergeTree table engine -->\n    <graphite_rollup_example>\n        <pattern>\n            <regexp>click_cost</regexp>\n            <function>any</function>\n            <retention>\n                <age>0</age>\n                <precision>3600</precision>\n            </retention>\n            <retention>\n                <age>86400</age>\n                <precision>60</precision>\n            </retention>\n        </pattern>\n        <default>\n            <function>max</function>\n            <retention>\n                <age>0</age>\n                <precision>60</precision>\n            </retention>\n            <retention>\n                <age>3600</age>\n                <precision>300</precision>\n            </retention>\n            <retention>\n                <age>86400</age>\n                <precision>3600</precision>\n            </retention>\n        </default>\n    </graphite_rollup_example>\n\n    <!-- Directory in <clickhouse-path> containing schema files for various input formats.\n         The directory will be created if it doesn't exist.\n      -->\n    <format_schema_path>/data/clickhouse/format_schemas/</format_schema_path>\n\n   \n\n    <!-- Uncomment to use query masking rules.\n        name - name for the rule (optional)\n        regexp - RE2 compatible regular expression (mandatory)\n        replace - substitution string for sensitive data (optional, by default - six asterisks)\n    <query_masking_rules>\n        <rule>\n            <name>hide SSN</name>\n            <regexp>\\b\\d{3}-\\d{2}-\\d{4}\\b</regexp>\n            <replace>000-00-0000</replace>\n        </rule>\n    </query_masking_rules>\n    -->\n\n    <!-- Uncomment to disable ClickHouse internal DNS caching. -->\n    <!-- <disable_internal_dns_cache>1</disable_internal_dns_cache> -->\n</yandex>\n\n```\n\n### 基础配置 /data/clickhouse/conf/config.d/config.base.xml\n\n会覆盖`/data/clickhouse/conf/config.xml`, 每个节点都一样\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n\n    <http_port>8123</http_port>\n    <tcp_port>9000</tcp_port>\n    <listen_host>10.64.138.24</listen_host>\n    <interserver_http_port>9009</interserver_http_port>\n    <interserver_http_host>10.64.138.24</interserver_http_host>\n    <path>/data1/clickhouse</path>\n    <tmp_path>/data1/clickhouse/tmp/</tmp_path>\n    <max_concurrent_queries>500</max_concurrent_queries>\n    <max_table_size_to_drop>0</max_table_size_to_drop>\n    <max_partition_size_to_drop>0</max_partition_size_to_drop>\n    <zookeeper replace=\"replace\">\n        <node index=\"0\">\n            <host>10.64.148.133</host>\n            <port>2181</port>\n        </node>\n        <node index=\"1\">\n            <host>10.64.148.134</host>\n            <port>2181</port>\n        </node>\n        <node index=\"1\">\n            <host>10.64.148.135</host>\n            <port>2181</port>\n        </node>\n    </zookeeper>\n</yandex>\n```\n\n### 集群分片配置 /data/clickhouse/conf/config.d/config.disks.xml\n\n每个节点都一样\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n <storage_configuration>\n        <disks>\n            <fast>\n                <path>/dev/shm/clickhouse/</path>\n                <!-- 10G -->\n             <keep_free_space_bytes>10737418240</keep_free_space_bytes>\n            </fast>\n            <default>\n             <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </default>\n            <data3>\n                <path>/data3/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data3>\n            <data4>\n                <path>/data4/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data4>\n            <data5>\n                <path>/data5/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data5>\n           \n            <data7>\n                <path>/data7/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data7>\n            <data8>\n                <path>/data8/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data8>\n         \n        </disks>\n        <policies>\n            <hdd_in_order>\n                <volumes>\n                    <hot>\n                       \t<disk>default</disk>\n                        <disk>data3</disk>\n\t\t\t\t\t\t<max_data_part_size_bytes>1073741824</max_data_part_size_bytes>\n                    </hot>\n                     <cold>\n                        <disk>data4</disk>\n                        <disk>data5</disk>\n\t\t\t\t\t\t<max_data_part_size_bytes>1073741824</max_data_part_size_bytes>\n                    </cold>\n                    <archive>\n                        <disk>data7</disk>\n                        <disk>data8</disk>\n                    </archive>\n                </volumes>\n                <move_factor>0.2</move_factor>\n            </hdd_in_order>\n            <memory_cache>\n                <volumes>\n                    <memory>\n                        <disk>fast</disk>\n                    </memory>\n                </volumes>\n            </memory_cache>\n        </policies>\n    </storage_configuration>\n</yandex>\n```\n\n###   宏定义 /data/clickhouse/conf/config.d/config.macros.xml\n\n用于执行建表语句时，可以替换成具体的值，不同机器节点不一样:\n\n由于ck的同个节点只能存一个分片的一个副本，所以4台机器可以实现2分片2副本\n\n对于节点1：10.64.148.133   存储分片01 副本 01， shard=01  replica=c1-01-01(表示集群c1, 01分片, 01副本)\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n    <macros>\n        <shard>01</shard>\n        <replica>c1-01-01</replica>\n    </macros>\n</yandex>\n```\n\n对于节点2：10.64.148.134   存储分片01 副本 02， shard=01  replica=c1-01-02(表示集群c1, 01分片, 02副本)\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n    <macros>\n        <shard>01</shard>\n        <replica>c1-01-02</replica>\n    </macros>\n</yandex>\n```\n\n对于节点3：10.64.148.135   存储分片02 副本 01， shard=02  replica=c1-02-01(表示集群c1, 02分片, 01副本)\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n    <macros>\n        <shard>02</shard>\n        <replica>c1-02-01</replica>\n    </macros>\n</yandex>\n```\n\n对于节点4：10.64.138.24   存储分片02 副本 02， shard=02 replica=c1-02-02(表示集群c1, 02分片, 02副本)\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n    <macros>\n        <shard>02</shard>\n        <replica>c1-02-02</replica>\n    </macros>\n</yandex>\n```\n\n由上面配置可以看到replica的分布规律，其中shard表示分片编号；replica是副本标识，这里使用了c1-{shard}-{replica}的表示方式，比如c1-02-1表示c1集群的02分片下的1号副本，这样既非常直观的表示又唯一确定副本. \n\n### 集群配置 /data/clickhouse/conf/config.d/config.remote_servers.xml\n\n这里指定了两个分片，每个分片两个副本。\n\n```\n<?xml version=\"1.0\"?>\n<yandex>\n    <remote_servers replace=\"replace\">\n        <base>\n            <shard>\n                <weight>1</weight>\n                <internal_replication>true</internal_replication>\n\n                <replica>\n                    <host>10.64.148.133</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>x</password>\n                </replica>\n                <replica>\n                    <host>10.64.148.134</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>x</password>\n                </replica>\n\n            </shard>\n            <shard>\n                <weight>1</weight>\n                <internal_replication>true</internal_replication>\n                <replica>\n                    <host>10.64.148.135</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>x</password>\n                </replica>\n                <replica>\n                    <host>10.68.138.24</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>x</password>\n                </replica>\n\n            </shard>\n        </base>\n    </remote_servers>\n</yandex>\n```\n\n## 启动clickhouse-server进程\n\n执行bash start_ch.sh start\n\n## 测试\n\n### 建表\n\n```\ncreate table t_sample3 on cluster base (id UInt64, its DateTime default now()) Engine=ReplicatedMergeTree('/clickhouse/tables/{shard}/table_name','{replica}') order by (its,id);\n\nCREATE TABLE dis_sample3 on cluster base AS t_sample3  ENGINE = Distributed(base, default, t_sample3, rand())\n```\n\n插入数据：\n\n```\n在节点1执行\n\ninsert into dis_sample3 (id) select * from numbers(10000000);\n结果为：\nip-10-64-148-133.yygamedev.com :) select count() from dis_sample3\n\nSELECT count()\nFROM dis_sample3\n\n┌──count()─┐\n│ 10000000 │\n└──────────┘\n\n1 rows in set. Elapsed: 0.146 sec.\n```\n\n查看每个节点part_log\n\n节点1：\n\n```\nip-10-64-148-133.yygamedev.com :) SELECT event_type, path_on_disk FROM system.part_log where table='t_sample3'\n\nSELECT\n    event_type,\n    path_on_disk\nFROM system.part_log\nWHERE table = 't_sample3'\n\n┌─event_type─┬─path_on_disk────────────────────────────────────────┐\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_0_0_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_1_1_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_2_2_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_3_3_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_4_4_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_5_5_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_6_6_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_7_7_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_8_8_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_9_9_0/ │\n│ MergeParts │ /data1/clickhouse/data/default/t_sample3/all_0_5_1/ │\n└────────────┴─────────────────────────────────────────────────────┘\n\n11 rows in set. Elapsed: 0.019 sec.\n```\n\n节点2：\n\n```\nip-10-64-148-134.yygamedev.com :) SELECT event_type, path_on_disk FROM system.part_log where table='t_sample3'\n\nSELECT\n    event_type,\n    path_on_disk\nFROM system.part_log\nWHERE table = 't_sample3'\n\n┌─event_type───┬─path_on_disk────────────────────────────────────────┐\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_0_0_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_1_1_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_2_2_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_3_3_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_4_4_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_5_5_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_6_6_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_7_7_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_8_8_0/ │\n│ MergeParts   │ /data1/clickhouse/data/default/t_sample3/all_0_5_1/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_9_9_0/ │\n└──────────────┴─────────────────────────────────────────────────────┘\n\n11 rows in set. Elapsed: 0.003 sec.\n```\n\n节点3：\n\n```\nip-10-64-148-135.yygamedev.com :) SELECT event_type, path_on_disk FROM system.part_log where table='t_sample3'\n\nSELECT\n    event_type,\n    path_on_disk\nFROM system.part_log\nWHERE table = 't_sample3'\n\n┌─event_type─┬─path_on_disk────────────────────────────────────────┐\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_0_0_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_1_1_0/ │\n└────────────┴─────────────────────────────────────────────────────┘\n┌─event_type─┬─path_on_disk────────────────────────────────────────┐\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_2_2_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_3_3_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_4_4_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_5_5_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_6_6_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_7_7_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_8_8_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_9_9_0/ │\n│ MergeParts │ /data1/clickhouse/data/default/t_sample3/all_0_6_1/ │\n└────────────┴─────────────────────────────────────────────────────┘\n```\n\n节点4\n\n```\nip-10-64-138-24.yygamedev.com :) SELECT event_type, path_on_disk FROM system.part_log where table='t_sample3'\n\nSELECT\n    event_type,\n    path_on_disk\nFROM system.part_log\nWHERE table = 't_sample3'\n\n┌─event_type───┬─path_on_disk────────────────────────────────────────┐\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_0_0_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_1_1_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_2_2_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_3_3_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_4_4_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_5_5_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_6_6_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_7_7_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_8_8_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_9_9_0/ │\n│ MergeParts   │ /data1/clickhouse/data/default/t_sample3/all_0_6_1/ │\n└──────────────┴─────────────────────────────────────────────────────┘\n\n11 rows in set. Elapsed: 0.003 sec.\n```\n\n可以看到，节点2和节点4都是DownloadPart\n\n\n\n## 停掉一个节点，对查询没有影响","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"union all并行查询提速","url":"/clickhouse/optimize-unionall/","content":"# union all并行查询提速\n\n<!--more-->\n# 原查询是：\n\n```\nSELECT\n    a.count AS reqcount,\n    b.count AS tenseccount,\n    c.count AS thirtyseccount,\n    d.count AS onemincount,\n    e.count AS fivemincount,\n    f.count AS failcount,\n    g.count AS verifycount,\n    h.count AS verifysuccesscount,\n    i.count AS cost,\n    j.count AS onetofivemincount\nFROM\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS a\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime < 10000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS b\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 10000) AND (g.receiptusetime < 30000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS c\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 30000) AND (g.receiptusetime < 60000) AND (g.receiptusetime >= 60000) AND (g.receiptusetime < 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS d\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS e\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND ((g.status = '0') OR (g.status = '2')) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS f\n,\n(\n    SELECT count(0) AS count\n    FROM dis_sms_captcha AS c\n    WHERE (1 = 1) AND (c.appid = '5001') AND (c.its >= 1587312000) AND (c.its <= 1587484800)\n) AS g\n,\n(\n    SELECT count(0) AS count\n    FROM dis_sms_captcha AS c\n    WHERE (1 = 1) AND (c.appid = '5001') AND (c.mobileverifycount > '0') AND (c.its >= 1587312000) AND (c.its <= 1587484800)\n) AS h\n,\n(\n    SELECT sum(toDecimal128(notEmpty(price), 4)) AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.appid = '5001') AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS i\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 60000) AND (g.receiptusetime < 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS j\nWHERE 1 = 1\n```\n\n# 查询是串行的，执行结果为:\n\n```\n1 rows in set. Elapsed: 6.443 sec. Processed 8.20 million rows, 47.30 MB (1.27 million rows/s., 7.34 MB/s.)\n```\n\n# 使用union all是查询可以并行，sql变为\n```\nSELECT\n   name,count\nFROM (\n(\n    SELECT 'a' as name,count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'b',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime < 10000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'c',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 10000) AND (g.receiptusetime < 30000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) \nunion all\n(\n    SELECT 'd',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 30000) AND (g.receiptusetime < 60000) AND (g.receiptusetime >= 60000) AND (g.receiptusetime < 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) \nunion all\n(\n    SELECT 'e',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'f',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND ((g.status = '0') OR (g.status = '2')) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'g',count(0) AS count\n    FROM dis_sms_captcha AS c\n    WHERE (1 = 1) AND (c.appid = '5001') AND (c.its >= 1587312000) AND (c.its <= 1587484800)\n) \nunion all\n(\n    SELECT 'h',count(0) AS count\n    FROM dis_sms_captcha AS c\n    WHERE (1 = 1) AND (c.appid = '5001') AND (c.mobileverifycount > '0') AND (c.its >= 1587312000) AND (c.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'i',sum(toDecimal128(notEmpty(price), 4)) AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.appid = '5001') AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'j',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 60000) AND (g.receiptusetime < 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\n)\n\n```\n\n# 查询结果为：\n```\n10 rows in set. Elapsed: 0.376 sec. Processed 8.20 million rows, 47.30 MB (21.83 million rows/s., 125.94 MB/s.)\n```","tags":["sql优化"],"categories":["clickhouse"]},{"title":"clickhouse集群迁移实践","url":"/clickhouse/qianyi/","content":"\n# 集群迁移实践\n\n# 背景\n\n\n现有的ck集群没有副本，磁盘是12块盘的stat盘，存在磁盘故障导致数据丢失的风险，而依赖zk的双副本策略又由于zk性能存在瓶颈影响集群的可用性，故而使用带三副本的高效云盘替代stat盘，规避数据丢失的风险。\n\n当前ck的写入程序使用的是统一的域名，由域名查询到对应的ip节点来建立tcp连接。对于ck的查询，使用的是内部的一个代理，该代理配置了集群的ip，由代理去轮询ip进行查询。\n\n在数据迁移的过程中，需要保证集群写入和查询都不受影响，关键在于控制好查询和写入的节点。\n\n<!--more-->\n# 迁移方案\n\n## 1. 新节点安装ck，同步元数据，元数据管理覆盖新节点ip\n\n新节点安装时，配置文件的remote_servers中的集群配置和旧集群一致，新节点只是做一个查询转发。\n\n从旧节点中导出非system表的建表语句，由于一份表对应着一个本地表和一个分布式表，因此需要先创建完本地表。\n\n```\n导出本地表\necho \"select create_table_query||';'  from system.tables where database != 'system' and engine!='Distributed' order by name desc\" | /data/clickhouse/bin/clickhouse-client --password xxx --port 9000 > localtable.txt\n\n导出分布式表的ddl\necho \"select create_table_query||';'  from system.tables where database != 'system' and engine='Distributed' order by name desc\" | /data/clickhouse/bin/clickhouse-client --password xxx --port 9000 > distable.txt\n\n```\n\n将建表语句发送到新节点执行\n\n```\n执行本地表的ddl\n./sendfile.sh ckip.txt localtable.txt distable.txt\n./runcmd.sh ckip.txt \"/data/clickhouse/bin/clickhouse-client  --password xxx -mn < /home/jinyaqia/ck_tool/localtable.txt\"\n\n执行分布式表的ddl\n./runcmd.sh ckip.txt \"/data/clickhouse/bin/clickhouse-client  --password xxx -mn < /home/jinyaqia/ck_tool/distable.txt\"\n```\n\n其中\n\n```\n# sendfile.sh\n# 用于发送文件\n#!/usr/bin/env bash\n\nfile=''\narray=\"$@\"\ni=0\nfor el in $array\ndo\n    if [ ${i} -ne 0 ]\n\tthen\n\t\tfile=$file' '${el}\n    fi\n\tlet i++\ndone\n\nfor ip in $(cat $1|grep -v \"^#.*\\|^$\")\ndo\n    cmd=\"scp ${file} $ip:~/ck_tool/\"\n\t$cmd &\n\techo $ip,$cmd\ndone\n\nwait\necho 'done'\n```\n\n```\n# runcmd.sh\n# 用于执行命令\n#!/usr/bin/env bash\n\nfor ip in $(cat $1|grep -v \"^#.*\\|^$\")\ndo\n\techo $ip\n    ssh $ip \"${2}\" &\n    echo \"end ${ip}\"\ndone\n\nwait\n```\n\n## 2. 配置查询代理，从新节点查询数据\n## 3. 滚动迁移\n### 3.1 先停掉待迁移节点的写入\n域名dns管理去掉待迁移节点，这里需要写入端没有直接缓存ck节点ip，确保在数据复制的过程中，新的数据不会写入待迁移的节点\n### 3.2 ck节点一对一复制数据\n采用clickhouse copier的方式进行迁移，生成所有本地表的迁移配置，调用迁移脚本进行数据复制\n\n1. 导出需要进行迁移的表，可以从system.tables中查询并导出\n\n```\necho \"select database||'.'||name,engine_full  from system.tables where database != 'system' and engine not in ('Distributed','View','MaterializedView')  order by name desc\" | /data/clickhouse/bin/clickhouse-client --password xxx --port 9000 > dbtable.txt.all\n\n```\n\n导出的文件去掉.inner.的表，物化视图等普通本地表导完后再处理\n如果有转义的,如'\\，手工替换掉就可以了\n\n2. 将脚本和文件都上传到接口机，由接口机发送到新节点,脚本内容后文给出\n\n```\ndbtable.txt.all  需要迁移的表\ndbtable.txt.todo 需要执行迁移的表\ntable_copy_copier.py  迁移脚本\ntable_check.py  数据一致性检查脚本\nget_error.py  处理迁移异常的表\nipmap.txt     旧节点复制到新节点的ip对应关系\ncopier_template.xml 复制工具配置模板\nzk.xml  复制工具zk配置\n\n发送命令\n./sendfile.sh ckip.txt dbtable.txt.all dbtable.txt.todo table_copy_copier.py table_check.py get_error.py copier_template.xml zk.xml\n```\n\n3. 执行复制\n\n```\nnohup python3 table_copy_copier.py 2>&1 >copy.log &\n```\n\n4. 校对、检查和重跑\n\n```\n批量校对\npython3 table_check.py\npython3 get_error.py\n\n有异常的表会写入dbtable.txt.1\n其中新节点数据条数少的写入error.txt.less\n新节点数据条数多了的写入error.txt.more\n\n手动核对下dbtable.txt.1里的表迁移异常的原因，可以查clickhouse-copier的日志，如果要重跑，则生成新的dbtable.txt.todo, 删掉zk中对应表的节点，然后调用table_copy_copier.py重新复制\n\n```\n### 3.3 更新ck集群配置remote_servers，增加新节点，去掉旧节点，等待ck server自动加载更新配置，此时查询会落到新节点上\n\n### 3.4 配置域名，使数据写入新节点\n\n## 4. 退回机器\n\n# 注意点\n\n1. 该方案在数据复制的时候要求旧节点和新节点都不会写数据，这个需要写入的时候不写分布式表\n2. 复制任务重跑时，需要删掉zk节点的数据\n3. 由于复制任务会在zk上创建比较多的节点数，为了降低zk的延迟，不要用ck集群的zk，可以单独搭建一个zk集群\n4. table_copy_copier.py调用 clickhouse copier命令后，即使主进程关闭了，copier任务也会一直运行\n5. table_copy_copier.py配置了60个子线程用于调用copier任务，如果要降低带宽的使用，可以调低并行度\n\n\n# 附录：\n## table_copy_copier.py\n\n```python\nimport sys,json;\nimport socket;\nimport os;\nfrom concurrent.futures import ThreadPoolExecutor,as_completed\nimport queue,threading;\nimport time,functools\n\ndef get_time():\n    time_now=int(time.time())\n    time_local=time.localtime(time_now)\n    return time.strftime(\"%Y-%m-%d %H:%M:%S\",time_local)\n   \ndef exec_command(fo,db,table,command_template,ip,task_file):\n    try:\n        task_path='/copytask/{}/{}/{}'.format(ip,db,table)\n        base_dir=table\n        if not os.path.exists(base_dir):\n            os.makedirs(base_dir)\n        command=command_template.format(task_path=task_path,base_dir=base_dir,task_file=task_file)\n        print(\"start thread num={} command={} pid={}\".format(threading.get_ident(),command,os.getpid()))\n        f = os.popen(command)\n        print(\"done thread num={} command={} pid={}\".format(threading.get_ident(),command,os.getpid()))\n        res=\"start {}.{} time={}\\n\".format(db,table,get_time())\n        fo.write(res)\n        fo.flush()\n    except:\n        error=\"start {}.{} error\\n\".format(db,table)\n        fo.write(error)\n        fo.flush()\n    return f\n\ndef read_db_table(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read().split('\\n')\n    finally:\n        f.close()\n    return lines\n\ndef read_file(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read()\n    finally:\n        f.close()\n    return lines\n\ndef write_file(file_name,content):\n    try:\n        f = open(file_name,\"w+\")\n        f.write(content)\n    finally:\n        f.close()\n\ndef get_local_ip():\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect(('8.8.8.8', 80))\n        localIp = s.getsockname()[0]\n    finally:\n        s.close()\n    return localIp\n\ndef task_done_callback(future,dbtable_str):\n    try:\n        data = future.result()\n        res=\"done {} time {}\\n\".format(dbtable_str,get_time())\n    except CancelledError:\n        res=\"error {} time {}\\n\".format(dbtable_str,get_time())\n    finally:\n        fo = open(\"result.txt\",\"a+\")\n        fo.write(res)\n        fo.flush()\n        print('result',data.read())\n        fo.close()\n\n\nif __name__ == '__main__':\n\n    ipmap=json.loads(read_file(\"ipmap.txt\"))\n    print(ipmap)\n    localIp = get_local_ip()\n    print(localIp)\n    remoteIp = ipmap[localIp]\n    print(remoteIp)\n    # copier template\n    copier_template = read_file(\"copier_template.xml\")\n    print(copier_template)\n    # db.table needs to copy\n    lines = filter(None,read_db_table(\"dbtable.txt.todo\"))\n    print(lines)\n    command_template=\"/data/clickhouse/bin/clickhouse copier --config zk.xml --task-path {task_path} --task-file {task_file} --base-dir {base_dir}\"\n    result_file = open(\"task_log.txt\",\"a+\")\n    pool = ThreadPoolExecutor(60)\n    waitQueue = queue.Queue()\n    for line in lines:\n        waitQueue.put(line)\n    \n    while(waitQueue.qsize()>0):\n        line_arr = waitQueue.get().split('\\t')\n        com=line_arr[0]\n        task_file=\"task_{}.xml\".format(com)\n        dbtable=com.split(\".\")\n        content=copier_template.format(source_host=remoteIp,target_host=localIp,database=dbtable[0],table=dbtable[1],engine=line_arr[1])\n        write_file(task_file,content)\n        task = pool.submit(exec_command, result_file, dbtable[0],dbtable[1], command_template, remoteIp, task_file).add_done_callback(functools.partial(task_done_callback,dbtable_str=com))\n        \n```\n\n## table_check.py\n\n```\nimport sys,json;\nimport socket;\nimport os;\nfrom concurrent.futures import ThreadPoolExecutor,as_completed,CancelledError\nimport queue,threading;\nimport time,functools\n\ndef get_time():\n    time_now=int(time.time())\n    time_local=time.localtime(time_now)\n    return time.strftime(\"%Y-%m-%d %H:%M:%S\",time_local)\n   \ndef exec_command(fo, dbtable_str,sql):\n    \n    try:        \n        dbtable=dbtable_str.split(\".\")\n        command=sql.format(db=dbtable[0],table=dbtable[1])\n        print(\"start thread num={} command={} pid={}\".format(threading.get_ident(),command,os.getpid()))\n        f = os.popen(command)\n        res=\"start {} time={}\\n\".format(dbtable_str,get_time())\n        fo.write(res)\n        fo.flush()\n    except:\n        error=\"start {} error\\n\".format(dbtable_str)\n        fo.write(error)\n        fo.flush()\n    return f\n\ndef read_db_table(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read().split('\\n')\n    finally:\n        f.close()\n    return lines\n\ndef read_file(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read()\n    finally:\n        f.close()\n    return lines\n\ndef get_local_ip():\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect(('8.8.8.8', 80))\n        localIp = s.getsockname()[0]\n    finally:\n        s.close()\n    return localIp\n\ndef task_done_callback(future,fo, dbtable_str,engine):\n    res=\"\"\n    try:\n        data = future.result()\n        counts = data.read().split('\\n')\n        line1= counts[0].split('\\t')\n        line2=counts[1].split('\\t')\n        count1=count2=0\n        if(line1[0]=='a'):\n            count1=line1[1]\n            count2=line2[1]\n        else:\n            count1=line2[1]\n            count2=line1[1]\n        res=\"done|{}\\t{}|result@{}@{}@{}\\n\".format(dbtable_str,engine,count1,count2,int(count1)-int(count2))\n    except CancelledError:\n        res=\"error {} \\n\".format(dbtable_str)\n    finally:\n        fo.write(res)\n        fo.flush()\n        print('result',res)\n    \n\n\nif __name__ == '__main__':\n    ipmap=json.loads(read_file(\"ipmap.txt\"))\n    print(ipmap)\n    localIp = get_local_ip()\n    print(localIp)\n    remoteIp = ipmap[localIp]\n    print(remoteIp)\n    sql=\"echo \\\"select 'a',count() from remote('\"+remoteIp+\"','{db}','{table}','default','IARYxRcr') union all select 'b',count() from {db}.{table};\\\" |/data/clickhouse/bin/clickhouse-client  --password IARYxRcr\"\n    lines = filter(None,read_db_table(\"dbtable.txt.all\"))\n    print(lines)\n    fo = open(\"checkresult.txt\",\"w+\")\n    fo1 = open(\"checkresult1.txt\",\"w+\")\n    pool = ThreadPoolExecutor(60)\n    waitQueue = queue.Queue()\n    for line in lines:\n        waitQueue.put(line)\n    \n    while(waitQueue.qsize()>0):\n        line_arr = waitQueue.get().split('\\t')\n        com=line_arr[0]\n        engine=line_arr[1]\n        task = pool.submit(exec_command, fo, com, sql).add_done_callback(functools.partial(task_done_callback,dbtable_str=com,fo=fo1,engine=engine))\n        \n```\n\n## get_error.py\n\n```\nimport sys\nimport json\nimport socket\nimport os\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport queue\nimport threading\nimport time\nimport functools\n\n\ndef read_file(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read().split('\\n')\n    finally:\n        f.close()\n    return lines\n\n\nif __name__ == '__main__':\n    fo = open(\"dbtable.txt.1\", \"w+\")\n    done = open('finish.txt', \"w+\")\n    lines = read_file(\"checkresult1.txt\")\n    # 比旧节点少了\n    err_less = open('error.txt.less', 'w+')\n    # 比旧节点多了\n    err_more = open('error.txt.more', 'w+')\n    todo = 0\n    to_delete = 0\n    finish = 0\n    for line in lines:\n        if(line != ''):\n            line_arr = line.split('|')\n            print(line_arr)\n            result = line_arr[2].split('@')\n            dbtable = line_arr[1].split('\\t')[0]\n            old=int(result[1])\n            new = int(result[2])\n            diff=int(result[3])\n    \n            if(diff != 0 and new == 0):\n                todo += 1\n                fo.write(line_arr[1]+'\\n')\n            elif(diff != 0 and new != 0):\n                todo += 1\n                to_delete += 1\n                fo.write(line_arr[1]+'\\n')\n            else:\n                finish += 1\n                done.write(dbtable+'\\n')\n\n            if(diff>0):\n                err_less.write(\"{} {}\\n\".format(dbtable,result))\n            elif(diff<0):\n                err_more.write(\"{} {}\\n\".format(dbtable,result))\n\n    print(\"finish={}, todo={}\".format(finish, to_delete, todo))\n    fo.close()\n    done.close()\n    err_less.close()\n    err_more.close()\n\n```\n\n## copier_template.xml\n\n```\n<yandex>\n    <remote_servers>\n        <source_cluster>\n            <shard>\n                <internal_replication>false</internal_replication>\n                    <replica>\n                        <host>{source_host}</host>\n                        <port>9000</port>\n                        <user>default</user>\n                        <password>IARYxRcr</password>\n                        <secure>0</secure>\n                    </replica>\n            </shard>\n        </source_cluster>\n\n        <destination_cluster>\n            <shard>\n                <internal_replication>false</internal_replication>\n                    <replica>\n                        <host>{target_host}</host>\n                        <port>9000</port>\n                        <user>default</user>\n                        <password>IARYxRcr</password>\n                        <secure>0</secure>\n                    </replica>\n            </shard>89\n        </destination_cluster>\n    </remote_servers>\n\n    <max_workers>32</max_workers>\n\n    <!-- Setting used to fetch (pull) data from source cluster tables -->\n    <settings_pull>\n        <readonly>1</readonly>\n    </settings_pull>\n\n    <settings_push>\n        <readonly>0</readonly>\n    </settings_push>\n\n     <settings>\n        <connect_timeout>3</connect_timeout>\n         <insert_distributed_sync>1</insert_distributed_sync>\n    </settings>\n\n    <tables>\n        <table_hits>\n            <!-- Source cluster name (from <remote_servers/> section) and tables in it that should be copied -->\n            <cluster_pull>source_cluster</cluster_pull>\n            <database_pull>{database}</database_pull>\n            <table_pull>{table}</table_pull>\n\n            <!-- Destination cluster name and tables in which the data should be inserted -->\n            <cluster_push>destination_cluster</cluster_push>\n            <database_push>{database}</database_push>\n            <table_push>{table}</table_push>\n\n            <engine>\n            ENGINE={engine}\n            </engine>\n            <sharding_key>rand()</sharding_key>\n\n        </table_hits>\n    </tables>\n</yandex>\n```\n\n## ipmap.txt\n\n```\n{\n    \"10.69.28.1\": \"10.69.29.11\",\n    \"10.69.29.2\": \"10.69.28.22\",\n    \"10.69.29.3\": \"10.69.29.33\",\n    \"10.69.29.4\": \"10.69.29.44\"\n}\n```\n\n## zk.txt\n```\n<yandex>\n    <logger>\n        <level>trace</level>\n        <size>100M</size>\n        <count>3</count>\n    </logger>\n    <zookeeper>\n        <node index=\"0\">\n            <host>zkhost</host>\n            <port>2181</port>\n        </node>\n    </zookeeper>\n</yandex>\n```","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"404","url":"//404.html","content":"\n```\n  ██╗  ██╗ ██████╗ ██╗  ██╗    ███╗   ██╗ ██████╗ ████████╗\n  ██║  ██║██╔═████╗██║  ██║    ████╗  ██║██╔═══██╗╚══██╔══╝\n  ███████║██║██╔██║███████║    ██╔██╗ ██║██║   ██║   ██║\n  ╚════██║████╔╝██║╚════██║    ██║╚██╗██║██║   ██║   ██║\n       ██║╚██████╔╝     ██║    ██║ ╚████║╚██████╔╝   ██║\n       ╚═╝ ╚═════╝      ╚═╝    ╚═╝  ╚═══╝ ╚═════╝    ╚═╝\n\n      ███████╗ ██████╗ ██╗   ██╗███╗   ██╗██████╗\n      ██╔════╝██╔═══██╗██║   ██║████╗  ██║██╔══██╗\n      █████╗  ██║   ██║██║   ██║██╔██╗ ██║██║  ██║\n      ██╔══╝  ██║   ██║██║   ██║██║╚██╗██║██║  ██║\n      ██║     ╚██████╔╝╚██████╔╝██║ ╚████║██████╔╝\n      ╚═╝      ╚═════╝  ╚═════╝ ╚═╝  ╚═══╝╚═════╝\n```\n"},{"title":"about","url":"/about/index.html"},{"title":"分类","url":"/categories/index.html"},{"title":"Plugins","url":"/plugins/index.html","content":"\n### Hexo Plugins\n\n* [hexo-generator-searchdb](https://github.com/next-theme/hexo-generator-searchdb)\n* [hexo-filter-emoji](https://github.com/next-theme/hexo-filter-emoji)\n* [hexo-pangu](https://github.com/next-theme/hexo-pangu)\n* [hexo-filter-mathjax](https://github.com/next-theme/hexo-filter-mathjax)\n* [hexo-word-counter](https://github.com/next-theme/hexo-word-counter)\n\n### NexT Plugins\n\n* [hexo-next-three](https://github.com/next-theme/hexo-next-three)\n* [hexo-next-fireworks](https://github.com/next-theme/hexo-next-fireworks)\n* [hexo-next-exif](https://github.com/next-theme/hexo-next-exif)\n\n---\n\n* Visit the [Awesome NexT](https://github.com/next-theme/awesome-next) list for more plugins.\n"},{"title":"标签","url":"/tags/index.html"}]