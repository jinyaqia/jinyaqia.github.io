[{"url":"/flink/remote-debug/"},{"title":"clickhouse源码构建","url":"/clickhouse/build/","content":"\n# clickhouse源码构建\n\n## 概述\n- 官方文档：https://clickhouse.com/docs/en/development/build/\n- 源码分支 21.12\n- 环境依赖\n  - ubuntu: 16.04\n  - cmake: 3.20.6\n  - clang: clang-13\n\n<!-- more -->\n\n## 步骤\n\n- 准备源码: \n\n  clickhouse官方库为：https://github.com/ClickHouse/ClickHouse\n\n  `git clone -b 21.12 https://github.com/ClickHouse/ClickHouse.git`，可切换到其他具体的分支\n\n- 执行git submodule更新相关的依赖包，依赖包会下载到``ClickHouse/contrib`下\n\n  ```shell\n  git submodule sync\n  # 初始化并下载相关依赖\n  git submodule update --init --recursive \n  ```\n\n- 编译环境下载相关的依赖，如cmake，clang等\n\n- 将下面脚本`build.sh`放在ClickhouseHouse目录下，执行`bash build.sh`\n\n  ```\n  mkdir build-clang\n  cd build-clang\n  \n  je=1\n  tc=1\n  BT=Release\n  #BT=Debug\n  \n  export TMPDIR=/data/tmp\n  /opt/cmake-3.20.6-linux-x86_64/bin/cmake .. -DENABLE_JEMALLOC=${je}  -DENABLE_TCMALLOC=${tc} -DCMAKE_EXPORT_COMPILE_COMMANDS=1  -DENABLE_TESTS=OFF  -DCMAKE_BUILD_TYPE=${BT} -DENABLE_RDKAFKA=0 -DCMAKE_CXX_COMPILER=`which clang++-13` -DCMAKE_C_COMPILER=`which clang-13`\n  \n  if [ $? -ne 0 ]; then\n    echo \"cmake faild\"\n    exit 1\n  fi\n  \n  \n  ninja -j20\n  \n  \n  if [ $? -ne 0 ]; then\n    echo \"faild\"\n    exit 1\n  fi\n  ```\n\n- 编译完可执行文件在`build-clang/programs下`\n\n  ```\n  $ tree -L 1 programs\n  programs\n  ├── bash-completion\n  ├── benchmark\n  ├── clickhouse\n  ├── clickhouse-benchmark -> clickhouse\n  ├── clickhouse-client -> clickhouse\n  ├── clickhouse-compressor -> clickhouse\n  ├── clickhouse-copier -> clickhouse\n  ├── clickhouse-extract-from-config -> clickhouse\n  ├── clickhouse-format -> clickhouse\n  ├── clickhouse-git-import -> clickhouse\n  ├── clickhouse-keeper -> clickhouse\n  ├── clickhouse-keeper-converter -> clickhouse\n  ├── clickhouse-library-bridge\n  ├── clickhouse-local -> clickhouse\n  ├── clickhouse-obfuscator -> clickhouse\n  ├── clickhouse-odbc-bridge\n  ├── clickhouse-server -> clickhouse\n  ├── clickhouse-static-files-disk-uploader -> clickhouse\n  ├── client\n  ├── CMakeFiles\n  ├── cmake_install.cmake\n  ├── compressor\n  ├── copier\n  ├── CTestTestfile.cmake\n  ├── extract-from-config\n  ├── format\n  ├── git-import\n  ├── install\n  ├── keeper\n  ├── keeper-converter\n  ├── library-bridge\n  ├── local\n  ├── obfuscator\n  ├── odbc-bridge\n  ├── server\n  └── static-files-disk-uploader\n  ```\n\n## 碰到的问题\n\n各种包依赖，这个时候看报错日志，把相关依赖解决就行","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"ck集群运维注意事项","url":"/clickhouse/dev_notify/","content":"\n# ck集群运维注意事项\n\n<!-- more -->\n\n1. alter时，只需要在一个分片上执行即可\n2. 遇到zookeeper session expire删除不了表，可以先detach再attach\n3. ck mutation可以保证异步操作不会删除新写入的数据，可以通过system.mutations查看进度\n4. 多磁盘空出两块盘，单独部署zk,并且将zk的log和snapshot分开\n5. 遇到too many parts时，需要控制写入速度。sinker看看是不是负载没有好,可以 alter table t_dw_huya_page_pv_ck on cluster single modify setting parts_to_throw_insert=600 增大抛异常的值，但是本质上还是后台merge跟不上插入，可以增大io性能，或减少分区\n6. 没有副本的cluster,要设置 <internal_replication>false</internal_replication>\n7. 修改```<background_pool_size>64</background_pool_size> \n      <background_schedule_pool_size>64</background_schedule_pool_size>\n      <background_move_pool_size>32</background_move_pool_size>```调大后台执行线程数\n8. 修改```<max_partitions_per_insert_block>1000</max_partitions_per_insert_block>``` 解决 DB::Exception: Too many partitions for single INSERT block (more than 100)的问题\n9. 修改```<max_replicated_merges_in_queue>200</max_replicated_merges_in_queue>\n        <max_replicated_mutations_in_queue>200</max_replicated_mutations_in_queue>```\n解决  Number of queued merges (36) and part mutations (0) is greater than max_replicated_merges_in_queue (16)  的错误\n11. 查看集群事件和指标\nSELECT \n    event_time,\n    ProfileEvent_InsertQuery,\n    runningDifference(ProfileEvent_InsertQuery) ins_per_sec\nFROM system.metric_log\nWHERE event_date = today()\nORDER BY event_time DESC\nLIMIT 50;\nSELECT value\nFROM system.events\nWHERE event LIKE 'InsertQuery'\n\n12. 查看多少mutation没执行```select table,command,mutation_id,create_time,is_done,latest_fail_time,latest_fail_reason,parts_to_do from system.mutations where table= 't_oexp_precomputation_all_metric' order by create_time limit  10```\n13. 碰到 The local set of parts of table default.dwd_test doesn’t look like the set of parts in ZooKeeper 的问题，一般都是手动操作了表的ddl导致不同步，可以把metadata下的这个表先移除，让server可以正常启动，然后再重建表，数据会自动同步\n14.  select count() as c ,database||'.'||table from system.replication_queue where type='MUTATE_PART'  group by database,table order by c\n15.  select count() as c ,database||'.'||table from system.mutations where is_done=0 group by database,table order by c\n16.  遇到卡住的时候，可以先把卡住的表detach，然后批次attach回来","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"clickhouse集群2副本安装","url":"/clickhouse/install/","content":"\n\n\n# clickhouse集群2副本安装\n\n## 使用4台机器\n\n```\n节点1  10.64.148.133\n节点2  10.64.148.134\n节点3  10.64.148.135\n节点4  10.64.138.24\n```\n<!-- more -->\n## 配置路径\n\n```\nroot@ip-10-64-138-24:/data/clickhouse# tree\n.\n├── bin\n│   ├── clickhouse\n│   ├── clickhouse-client -> /data/clickhouse/bin/clickhouse\n│   └── clickhouse-server -> /data/clickhouse/bin/clickhouse\n├── conf\n│   ├── config.d\n│   │   ├── config.base.xml\n│   │   ├── config.disks.xml\n│   │   ├── config.macros.xml\n│   │   └── config.remote_servers.xml\n│   ├── config.xml\n│   ├── env.ini\n│   ├── users.d\n│   │   └── users.huya.xml\n│   └── users.xml\n├── format_schemas\n├── log\n│   ├── clickhouse-server.err.log\n│   ├── clickhouse-server.log\n│   ├── stderr.log\n│   └── stdout.log\n├── start_ch.sh\n├── tmp\n└── user_files\n```\n\n- `/data/clickhouse`为安装目录\n\n- `bin/`下为自行编译的clickhouse二进制文件\n\n- `conf/`下为ck的配置, `config.xml`为启动进程时默认加载的，\n  - `config.d/*.xml`为具体的子配置，具体包括\n    - `config.base.xml`: 基础配置\n    - `config.disks.xml`: 磁盘和存储策略配置\n    - `config.macros.xml`:宏定义配置\n    - `config.remote_servers.xml`: 集群配置\n  - `env.ini`： 环境配置\n- `users.d`: 存储用户配置，访问权限之类\n- `start_ch.sh`:启动脚本\n\n对应每个文件如下：\n\n### 默认启动配置 /data/clickhouse/conf/config.xml\n\n```\n<?xml version=\"1.0\"?>\n<!--\n  NOTE: User and query level settings are set up in \"users.xml\" file.\n-->\n<yandex>\n    <logger>\n        <!-- Possible levels: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105 -->\n        <level>trace</level>\n        <log>/data/clickhouse/log/clickhouse-server.log</log>\n        <errorlog>/data/clickhouse/log/clickhouse-server.err.log</errorlog>\n        <size>1000M</size>\n        <count>10</count>\n        <!-- <console>1</console> --> <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->\n    </logger>\n    <!--display_name>production</display_name--> <!-- It is the name that will be shown in the client -->\n    <http_port>8123</http_port>\n    <tcp_port>9000</tcp_port>\n    <mysql_port>9004</mysql_port>\n    <!-- For HTTPS and SSL over native protocol. -->\n    <!--\n    <https_port>8443</https_port>\n    <tcp_port_secure>9440</tcp_port_secure>\n    -->\n\n    <!-- Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71 -->\n    <openSSL>\n        <server> <!-- Used for https server AND secure tcp port -->\n            <!-- openssl req -subj \"/CN=localhost\" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt -->\n            <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>\n            <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile>\n            <!-- openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096 -->\n            <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>\n            <verificationMode>none</verificationMode>\n            <loadDefaultCAFile>true</loadDefaultCAFile>\n            <cacheSessions>true</cacheSessions>\n            <disableProtocols>sslv2,sslv3</disableProtocols>\n            <preferServerCiphers>true</preferServerCiphers>\n        </server>\n\n        <client> <!-- Used for connecting to https dictionary source -->\n            <loadDefaultCAFile>true</loadDefaultCAFile>\n            <cacheSessions>true</cacheSessions>\n            <disableProtocols>sslv2,sslv3</disableProtocols>\n            <preferServerCiphers>true</preferServerCiphers>\n            <!-- Use for self-signed: <verificationMode>none</verificationMode> -->\n            <invalidCertificateHandler>\n                <!-- Use for self-signed: <name>AcceptCertificateHandler</name> -->\n                <name>RejectCertificateHandler</name>\n            </invalidCertificateHandler>\n        </client>\n    </openSSL>\n\n    <!-- Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123 -->\n    <!--\n    <http_server_default_response><![CDATA[<html ng-app=\"SMI2\"><head><base href=\"http://ui.tabix.io/\"></head><body><div ui-view=\"\" class=\"content-ui\"></div><script src=\"http://loader.tabix.io/master.js\"></script></body></html>]]></http_server_default_response>\n    -->\n\n    <!-- Port for communication between replicas. Used for data exchange. -->\n    <interserver_http_port>9009</interserver_http_port>\n\n    <!-- Hostname that is used by other replicas to request this server.\n         If not specified, than it is determined analoguous to 'hostname -f' command.\n         This setting could be used to switch replication to another network interface.\n      -->\n    <!--\n    <interserver_http_host>example.yandex.ru</interserver_http_host>\n    -->\n\n    <!-- Listen specified host. use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere. -->\n    <!-- <listen_host>::</listen_host> -->\n    <!-- Same for hosts with disabled ipv6: -->\n    <!-- <listen_host>0.0.0.0</listen_host> -->\n\n    <!-- Default values - try listen localhost on ipv4 and ipv6: -->\n\n    <listen_host>0.0.0.0</listen_host>\n    <!-- <listen_host>127.0.0.1</listen_host> -->\n\n    <!-- Don't exit if ipv6 or ipv4 unavailable, but listen_host with this protocol specified -->\n    <!-- <listen_try>0</listen_try> -->\n\n    <!-- Allow listen on same address:port -->\n    <!-- <listen_reuse_port>0</listen_reuse_port> -->\n\n    <!-- <listen_backlog>64</listen_backlog> -->\n\n    <max_connections>4096</max_connections>\n    <keep_alive_timeout>3</keep_alive_timeout>\n\n    <!-- Maximum number of concurrent queries. -->\n    <max_concurrent_queries>500</max_concurrent_queries>\n\n    <!-- Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve\n         correct maximum value. -->\n    <!-- <max_open_files>262144</max_open_files> -->\n\n    <!-- Size of cache of uncompressed blocks of data, used in tables of MergeTree family.\n         In bytes. Cache is single for server. Memory is allocated only on demand.\n         Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).\n         Uncompressed cache is advantageous only for very short queries and in rare cases.\n      -->\n    <uncompressed_cache_size>8589934592</uncompressed_cache_size>\n\n    <!-- Approximate size of mark cache, used in tables of MergeTree family.\n         In bytes. Cache is single for server. Memory is allocated only on demand.\n         You should not lower this value.\n      -->\n    <mark_cache_size>5368709120</mark_cache_size>\n\n\n    <!-- Path to data directory, with trailing slash. -->\n    <path>/data/clickhouse/</path>\n\n    <!-- Path to temporary data for processing hard queries. -->\n    <tmp_path>/data/clickhouse/tmp/</tmp_path>\n\n    <!-- Policy from the <storage_configuration> for the temporary files.\n         If not set <tmp_path> is used, otherwise <tmp_path> is ignored.\n\n         Notes:\n         - move_factor              is ignored\n         - keep_free_space_bytes    is ignored\n         - max_data_part_size_bytes is ignored\n         - you must have exactly one volume in that policy\n    -->\n    <!-- <tmp_policy>tmp</tmp_policy> -->\n\n    <!-- Directory with user provided files that are accessible by 'file' table function. -->\n    <user_files_path>/data/clickhouse/user_files/</user_files_path>\n\n    <!-- Path to configuration file with users, access rights, profiles of settings, quotas. -->\n    <users_config>users.xml</users_config>\n\n    <!-- Default profile of settings. -->\n    <default_profile>default</default_profile>\n\n    <!-- System profile of settings. This settings are used by internal processes (Buffer storage, Distibuted DDL worker and so on). -->\n    <!-- <system_profile>default</system_profile> -->\n\n    <!-- Default database. -->\n    <default_database>default</default_database>\n\n    <!-- Server time zone could be set here.\n\n         Time zone is used when converting between String and DateTime types,\n          when printing DateTime in text formats and parsing DateTime from text,\n          it is used in date and time related functions, if specific time zone was not passed as an argument.\n\n         Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.\n         If not specified, system time zone at server startup is used.\n\n         Please note, that server could display time zone alias instead of specified name.\n         Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC.\n    -->\n    <timezone>Asia/Shanghai</timezone>\n\n    <!-- You can specify umask here (see \"man umask\"). Server will apply it on startup.\n         Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).\n    -->\n    <!-- <umask>022</umask> -->\n\n    <!-- Perform mlockall after startup to lower first queries latency\n          and to prevent clickhouse executable from being paged out under high IO load.\n         Enabling this option is recommended but will lead to increased startup time for up to a few seconds.\n    -->\n    <mlock_executable>false</mlock_executable>\n    <!-- Configuration of clusters that could be used in Distributed tables.\n         https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n      -->\n    <remote_servers incl=\"clickhouse_remote_servers\" >\n        <!-- Test only shard config for testing distributed storage -->\n    </remote_servers>\n\n    <!-- The list of hosts allowed to use in URL-related storage engines and table functions.\n        If this section is not present in configuration, all hosts are allowed.\n    -->\n    <remote_url_allow_hosts>\n        <!-- Host should be specified exactly as in URL. The name is checked before DNS resolution.\n            Example: \"yandex.ru\", \"yandex.ru.\" and \"www.yandex.ru\" are different hosts.\n                    If port is explicitly specified in URL, the host:port is checked as a whole.\n                    If host specified here without port, any port with this host allowed.\n                    \"yandex.ru\" -> \"yandex.ru:443\", \"yandex.ru:80\" etc. is allowed, but \"yandex.ru:80\" -> only \"yandex.ru:80\" is allowed.\n            If the host is specified as IP address, it is checked as specified in URL. Example: \"[2a02:6b8:a::a]\".\n            If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.\n        -->\n\n        <!-- Regular expression can be specified. RE2 engine is used for regexps.\n            Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter\n            (forgetting to do so is a common source of error).\n        -->\n    </remote_url_allow_hosts>\n\n    <!-- If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.\n         By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.\n         Values for substitutions are specified in /yandex/name_of_substitution elements in that file.\n      -->\n\n    <!-- ZooKeeper is used to store metadata about replicas, when using Replicated tables.\n         Optional. If you don't use replicated tables, you could omit that.\n\n         See https://clickhouse.yandex/docs/en/table_engines/replication/\n      -->\n\n    <zookeeper incl=\"zookeeper-servers\" optional=\"true\"/>\n\n    <!-- Substitutions for parameters of replicated tables.\n          Optional. If you don't use replicated tables, you could omit that.\n\n         See https://clickhouse.yandex/docs/en/table_engines/replication/#creating-replicated-tables\n      -->\n    <macros incl=\"macros\" optional=\"true\" />\n\n\n    <!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. -->\n    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>\n\n\n    <!-- Maximum session timeout, in seconds. Default: 3600. -->\n    <max_session_timeout>3600</max_session_timeout>\n\n    <!-- Default session timeout, in seconds. Default: 60. -->\n    <default_session_timeout>60</default_session_timeout>\n\n    <!-- Sending data to Graphite for monitoring. Several sections can be defined. -->\n    <!--\n        interval - send every X second\n        root_path - prefix for keys\n        hostname_in_path - append hostname to root_path (default = true)\n        metrics - send data from table system.metrics\n        events - send data from table system.events\n        asynchronous_metrics - send data from table system.asynchronous_metrics\n    -->\n    <!--\n    <graphite>\n        <host>localhost</host>\n        <port>42000</port>\n        <timeout>0.1</timeout>\n        <interval>60</interval>\n        <root_path>one_min</root_path>\n        <hostname_in_path>true</hostname_in_path>\n\n        <metrics>true</metrics>\n        <events>true</events>\n        <events_cumulative>false</events_cumulative>\n        <asynchronous_metrics>true</asynchronous_metrics>\n    </graphite>\n    <graphite>\n        <host>localhost</host>\n        <port>42000</port>\n        <timeout>0.1</timeout>\n        <interval>1</interval>\n        <root_path>one_sec</root_path>\n\n        <metrics>true</metrics>\n        <events>true</events>\n        <events_cumulative>false</events_cumulative>\n        <asynchronous_metrics>false</asynchronous_metrics>\n    </graphite>\n    -->\n\n    <!-- Serve endpoint fot Prometheus monitoring. -->\n    <!--\n        endpoint - mertics path (relative to root, statring with \"/\")\n        port - port to setup server. If not defined or 0 than http_port used\n        metrics - send data from table system.metrics\n        events - send data from table system.events\n        asynchronous_metrics - send data from table system.asynchronous_metrics\n    -->\n    <!--\n    <prometheus>\n        <endpoint>/metrics</endpoint>\n        <port>9363</port>\n\n        <metrics>true</metrics>\n        <events>true</events>\n        <asynchronous_metrics>true</asynchronous_metrics>\n    </prometheus>\n    -->\n\n    <!-- Query log. Used only for queries with setting log_queries = 1. -->\n    <query_log>\n        <!-- What table to insert data. If table is not exist, it will be created.\n             When query log structure is changed after system update,\n              then old table will be renamed and new table will be created automatically.\n        -->\n        <database>system</database>\n        <table>query_log</table>\n        <!--\n            PARTITION BY expr https://clickhouse.yandex/docs/en/table_engines/custom_partitioning_key/\n            Example:\n                event_date\n                toMonday(event_date)\n                toYYYYMM(event_date)\n                toStartOfHour(event_time)\n        -->\n        <partition_by>toYYYYMMDD(event_date)</partition_by>\n\n        <!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,\n             Example: <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024</engine>\n          -->\n\n        <!-- Interval of flushing data. -->\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </query_log>\n\n    <!-- Trace log. Stores stack traces collected by query profilers.\n         See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->\n    <trace_log>\n        <database>system</database>\n        <table>trace_log</table>\n\n        <partition_by>toYYYYMMDD(event_date)</partition_by>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </trace_log>\n\n    <!-- Query thread log. Has information about all threads participated in query execution.\n         Used only for queries with setting log_query_threads = 1. -->\n    <query_thread_log>\n        <database>system</database>\n        <table>query_thread_log</table>\n        <partition_by>toYYYYMM(event_date)</partition_by>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </query_thread_log>\n\n    <!-- Uncomment if use part log.\n         Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).\n         -->\n    <part_log>\n        <database>system</database>\n        <table>part_log</table>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n    </part_log>\n\n\n    <!-- Uncomment to write text log into table.\n         Text log contains all information from usual server log but stores it in structured and efficient way.\n         The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.\n           -->\n    <text_log>\n        <database>system</database>\n        <table>text_log</table>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n        <level>trace</level>\n    </text_log>\n\n\n    <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with \"collect_interval_milliseconds\" interval. -->\n    <metric_log>\n        <database>system</database>\n        <table>metric_log</table>\n        <flush_interval_milliseconds>7500</flush_interval_milliseconds>\n        <collect_interval_milliseconds>1000</collect_interval_milliseconds>\n    </metric_log>\n\n    <!-- Parameters for embedded dictionaries, used in Yandex.Metrica.\n         See https://clickhouse.yandex/docs/en/dicts/internal_dicts/\n    -->\n\n    <!-- Path to file with region hierarchy. -->\n    <!-- <path_to_regions_hierarchy_file>/opt/geo/regions_hierarchy.txt</path_to_regions_hierarchy_file> -->\n\n    <!-- Path to directory with files containing names of regions -->\n    <!-- <path_to_regions_names_files>/opt/geo/</path_to_regions_names_files> -->\n\n\n    <!-- Configuration of external dictionaries. See:\n         https://clickhouse.yandex/docs/en/dicts/external_dicts/\n    -->\n    <dictionaries_config>*_dictionary.xml</dictionaries_config>\n\n    <!-- Uncomment if you want data to be compressed 30-100% better.\n         Don't do that if you just started using ClickHouse.\n      -->\n    <compression incl=\"clickhouse_compression\">\n    <!--\n        <!- - Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used. - ->\n        <case>\n\n            <!- - Conditions. All must be satisfied. Some conditions may be omitted. - ->\n            <min_part_size>10000000000</min_part_size>        <!- - Min part size in bytes. - ->\n            <min_part_size_ratio>0.01</min_part_size_ratio>   <!- - Min size of part relative to whole table size. - ->\n\n            <!- - What compression method to use. - ->\n            <method>zstd</method>\n        </case>\n    -->\n    </compression>\n\n    <!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.\n         Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. -->\n    <distributed_ddl>\n        <!-- Path in ZooKeeper to queue with DDL queries -->\n        <path>/clickhouse/task_queue/ddl</path>\n\n        <!-- Settings from this profile will be used to execute DDL queries -->\n        <!-- <profile>default</profile> -->\n    </distributed_ddl>\n\n    <!-- Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h -->\n    <!--\n    <merge_tree>\n        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>\n    </merge_tree>\n    -->\n\n    <!-- Protection from accidental DROP.\n         If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.\n         If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.\n         By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.\n         The same for max_partition_size_to_drop.\n         Uncomment to disable protection.\n    -->\n    <max_table_size_to_drop>0</max_table_size_to_drop>\n    <max_partition_size_to_drop>0</max_partition_size_to_drop>\n\n    <!-- Example of parameters for GraphiteMergeTree table engine -->\n    <graphite_rollup_example>\n        <pattern>\n            <regexp>click_cost</regexp>\n            <function>any</function>\n            <retention>\n                <age>0</age>\n                <precision>3600</precision>\n            </retention>\n            <retention>\n                <age>86400</age>\n                <precision>60</precision>\n            </retention>\n        </pattern>\n        <default>\n            <function>max</function>\n            <retention>\n                <age>0</age>\n                <precision>60</precision>\n            </retention>\n            <retention>\n                <age>3600</age>\n                <precision>300</precision>\n            </retention>\n            <retention>\n                <age>86400</age>\n                <precision>3600</precision>\n            </retention>\n        </default>\n    </graphite_rollup_example>\n\n    <!-- Directory in <clickhouse-path> containing schema files for various input formats.\n         The directory will be created if it doesn't exist.\n      -->\n    <format_schema_path>/data/clickhouse/format_schemas/</format_schema_path>\n\n   \n\n    <!-- Uncomment to use query masking rules.\n        name - name for the rule (optional)\n        regexp - RE2 compatible regular expression (mandatory)\n        replace - substitution string for sensitive data (optional, by default - six asterisks)\n    <query_masking_rules>\n        <rule>\n            <name>hide SSN</name>\n            <regexp>\\b\\d{3}-\\d{2}-\\d{4}\\b</regexp>\n            <replace>000-00-0000</replace>\n        </rule>\n    </query_masking_rules>\n    -->\n\n    <!-- Uncomment to disable ClickHouse internal DNS caching. -->\n    <!-- <disable_internal_dns_cache>1</disable_internal_dns_cache> -->\n</yandex>\n\n```\n\n### 基础配置 /data/clickhouse/conf/config.d/config.base.xml\n\n会覆盖`/data/clickhouse/conf/config.xml`, 每个节点都一样\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n\n    <http_port>8123</http_port>\n    <tcp_port>9000</tcp_port>\n    <listen_host>10.64.138.24</listen_host>\n    <interserver_http_port>9009</interserver_http_port>\n    <interserver_http_host>10.64.138.24</interserver_http_host>\n    <path>/data1/clickhouse</path>\n    <tmp_path>/data1/clickhouse/tmp/</tmp_path>\n    <max_concurrent_queries>500</max_concurrent_queries>\n    <max_table_size_to_drop>0</max_table_size_to_drop>\n    <max_partition_size_to_drop>0</max_partition_size_to_drop>\n    <zookeeper replace=\"replace\">\n        <node index=\"0\">\n            <host>10.64.148.133</host>\n            <port>2181</port>\n        </node>\n        <node index=\"1\">\n            <host>10.64.148.134</host>\n            <port>2181</port>\n        </node>\n        <node index=\"1\">\n            <host>10.64.148.135</host>\n            <port>2181</port>\n        </node>\n    </zookeeper>\n</yandex>\n```\n\n### 集群分片配置 /data/clickhouse/conf/config.d/config.disks.xml\n\n每个节点都一样\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n <storage_configuration>\n        <disks>\n            <fast>\n                <path>/dev/shm/clickhouse/</path>\n                <!-- 10G -->\n             <keep_free_space_bytes>10737418240</keep_free_space_bytes>\n            </fast>\n            <default>\n             <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </default>\n            <data3>\n                <path>/data3/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data3>\n            <data4>\n                <path>/data4/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data4>\n            <data5>\n                <path>/data5/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data5>\n           \n            <data7>\n                <path>/data7/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data7>\n            <data8>\n                <path>/data8/clickhouse/</path>\n                <keep_free_space_bytes>572159652</keep_free_space_bytes>\n            </data8>\n         \n        </disks>\n        <policies>\n            <hdd_in_order>\n                <volumes>\n                    <hot>\n                       \t<disk>default</disk>\n                        <disk>data3</disk>\n\t\t\t\t\t\t<max_data_part_size_bytes>1073741824</max_data_part_size_bytes>\n                    </hot>\n                     <cold>\n                        <disk>data4</disk>\n                        <disk>data5</disk>\n\t\t\t\t\t\t<max_data_part_size_bytes>1073741824</max_data_part_size_bytes>\n                    </cold>\n                    <archive>\n                        <disk>data7</disk>\n                        <disk>data8</disk>\n                    </archive>\n                </volumes>\n                <move_factor>0.2</move_factor>\n            </hdd_in_order>\n            <memory_cache>\n                <volumes>\n                    <memory>\n                        <disk>fast</disk>\n                    </memory>\n                </volumes>\n            </memory_cache>\n        </policies>\n    </storage_configuration>\n</yandex>\n```\n\n###   宏定义 /data/clickhouse/conf/config.d/config.macros.xml\n\n用于执行建表语句时，可以替换成具体的值，不同机器节点不一样:\n\n由于ck的同个节点只能存一个分片的一个副本，所以4台机器可以实现2分片2副本\n\n对于节点1：10.64.148.133   存储分片01 副本 01， shard=01  replica=c1-01-01(表示集群c1, 01分片, 01副本)\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n    <macros>\n        <shard>01</shard>\n        <replica>c1-01-01</replica>\n    </macros>\n</yandex>\n```\n\n对于节点2：10.64.148.134   存储分片01 副本 02， shard=01  replica=c1-01-02(表示集群c1, 01分片, 02副本)\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n    <macros>\n        <shard>01</shard>\n        <replica>c1-01-02</replica>\n    </macros>\n</yandex>\n```\n\n对于节点3：10.64.148.135   存储分片02 副本 01， shard=02  replica=c1-02-01(表示集群c1, 02分片, 01副本)\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n    <macros>\n        <shard>02</shard>\n        <replica>c1-02-01</replica>\n    </macros>\n</yandex>\n```\n\n对于节点4：10.64.138.24   存储分片02 副本 02， shard=02 replica=c1-02-02(表示集群c1, 02分片, 02副本)\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yandex>\n    <macros>\n        <shard>02</shard>\n        <replica>c1-02-02</replica>\n    </macros>\n</yandex>\n```\n\n由上面配置可以看到replica的分布规律，其中shard表示分片编号；replica是副本标识，这里使用了c1-{shard}-{replica}的表示方式，比如c1-02-1表示c1集群的02分片下的1号副本，这样既非常直观的表示又唯一确定副本. \n\n### 集群配置 /data/clickhouse/conf/config.d/config.remote_servers.xml\n\n这里指定了两个分片，每个分片两个副本。\n\n```\n<?xml version=\"1.0\"?>\n<yandex>\n    <remote_servers replace=\"replace\">\n        <base>\n            <shard>\n                <weight>1</weight>\n                <internal_replication>true</internal_replication>\n\n                <replica>\n                    <host>10.64.148.133</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>x</password>\n                </replica>\n                <replica>\n                    <host>10.64.148.134</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>x</password>\n                </replica>\n\n            </shard>\n            <shard>\n                <weight>1</weight>\n                <internal_replication>true</internal_replication>\n                <replica>\n                    <host>10.64.148.135</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>x</password>\n                </replica>\n                <replica>\n                    <host>10.68.138.24</host>\n                    <port>9000</port>\n                    <user>default</user>\n                    <password>x</password>\n                </replica>\n\n            </shard>\n        </base>\n    </remote_servers>\n</yandex>\n```\n\n## 启动clickhouse-server进程\n\n执行bash start_ch.sh start\n\n## 测试\n\n### 建表\n\n```\ncreate table t_sample3 on cluster base (id UInt64, its DateTime default now()) Engine=ReplicatedMergeTree('/clickhouse/tables/{shard}/table_name','{replica}') order by (its,id);\n\nCREATE TABLE dis_sample3 on cluster base AS t_sample3  ENGINE = Distributed(base, default, t_sample3, rand())\n```\n\n插入数据：\n\n```\n在节点1执行\n\ninsert into dis_sample3 (id) select * from numbers(10000000);\n结果为：\nip-10-64-148-133.yygamedev.com :) select count() from dis_sample3\n\nSELECT count()\nFROM dis_sample3\n\n┌──count()─┐\n│ 10000000 │\n└──────────┘\n\n1 rows in set. Elapsed: 0.146 sec.\n```\n\n查看每个节点part_log\n\n节点1：\n\n```\nip-10-64-148-133.yygamedev.com :) SELECT event_type, path_on_disk FROM system.part_log where table='t_sample3'\n\nSELECT\n    event_type,\n    path_on_disk\nFROM system.part_log\nWHERE table = 't_sample3'\n\n┌─event_type─┬─path_on_disk────────────────────────────────────────┐\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_0_0_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_1_1_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_2_2_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_3_3_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_4_4_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_5_5_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_6_6_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_7_7_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_8_8_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_9_9_0/ │\n│ MergeParts │ /data1/clickhouse/data/default/t_sample3/all_0_5_1/ │\n└────────────┴─────────────────────────────────────────────────────┘\n\n11 rows in set. Elapsed: 0.019 sec.\n```\n\n节点2：\n\n```\nip-10-64-148-134.yygamedev.com :) SELECT event_type, path_on_disk FROM system.part_log where table='t_sample3'\n\nSELECT\n    event_type,\n    path_on_disk\nFROM system.part_log\nWHERE table = 't_sample3'\n\n┌─event_type───┬─path_on_disk────────────────────────────────────────┐\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_0_0_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_1_1_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_2_2_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_3_3_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_4_4_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_5_5_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_6_6_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_7_7_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_8_8_0/ │\n│ MergeParts   │ /data1/clickhouse/data/default/t_sample3/all_0_5_1/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_9_9_0/ │\n└──────────────┴─────────────────────────────────────────────────────┘\n\n11 rows in set. Elapsed: 0.003 sec.\n```\n\n节点3：\n\n```\nip-10-64-148-135.yygamedev.com :) SELECT event_type, path_on_disk FROM system.part_log where table='t_sample3'\n\nSELECT\n    event_type,\n    path_on_disk\nFROM system.part_log\nWHERE table = 't_sample3'\n\n┌─event_type─┬─path_on_disk────────────────────────────────────────┐\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_0_0_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_1_1_0/ │\n└────────────┴─────────────────────────────────────────────────────┘\n┌─event_type─┬─path_on_disk────────────────────────────────────────┐\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_2_2_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_3_3_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_4_4_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_5_5_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_6_6_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_7_7_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_8_8_0/ │\n│ NewPart    │ /data1/clickhouse/data/default/t_sample3/all_9_9_0/ │\n│ MergeParts │ /data1/clickhouse/data/default/t_sample3/all_0_6_1/ │\n└────────────┴─────────────────────────────────────────────────────┘\n```\n\n节点4\n\n```\nip-10-64-138-24.yygamedev.com :) SELECT event_type, path_on_disk FROM system.part_log where table='t_sample3'\n\nSELECT\n    event_type,\n    path_on_disk\nFROM system.part_log\nWHERE table = 't_sample3'\n\n┌─event_type───┬─path_on_disk────────────────────────────────────────┐\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_0_0_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_1_1_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_2_2_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_3_3_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_4_4_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_5_5_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_6_6_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_7_7_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_8_8_0/ │\n│ DownloadPart │ /data1/clickhouse/data/default/t_sample3/all_9_9_0/ │\n│ MergeParts   │ /data1/clickhouse/data/default/t_sample3/all_0_6_1/ │\n└──────────────┴─────────────────────────────────────────────────────┘\n\n11 rows in set. Elapsed: 0.003 sec.\n```\n\n可以看到，节点2和节点4都是DownloadPart\n\n\n\n## 停掉一个节点，对查询没有影响","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"clickhouse集群迁移实践","url":"/clickhouse/qianyi/","content":"\n# 集群迁移实践\n\n# 背景\n\n\n现有的ck集群没有副本，磁盘是12块盘的stat盘，存在磁盘故障导致数据丢失的风险，而依赖zk的双副本策略又由于zk性能存在瓶颈影响集群的可用性，故而使用带三副本的高效云盘替代stat盘，规避数据丢失的风险。\n\n当前ck的写入程序使用的是统一的域名，由域名查询到对应的ip节点来建立tcp连接。对于ck的查询，使用的是内部的一个代理，该代理配置了集群的ip，由代理去轮询ip进行查询。\n\n在数据迁移的过程中，需要保证集群写入和查询都不受影响，关键在于控制好查询和写入的节点。\n\n<!--more-->\n# 迁移方案\n\n## 1. 新节点安装ck，同步元数据，元数据管理覆盖新节点ip\n\n新节点安装时，配置文件的remote_servers中的集群配置和旧集群一致，新节点只是做一个查询转发。\n\n从旧节点中导出非system表的建表语句，由于一份表对应着一个本地表和一个分布式表，因此需要先创建完本地表。\n\n```\n导出本地表\necho \"select create_table_query||';'  from system.tables where database != 'system' and engine!='Distributed' order by name desc\" | /data/clickhouse/bin/clickhouse-client --password xxx --port 9000 > localtable.txt\n\n导出分布式表的ddl\necho \"select create_table_query||';'  from system.tables where database != 'system' and engine='Distributed' order by name desc\" | /data/clickhouse/bin/clickhouse-client --password xxx --port 9000 > distable.txt\n\n```\n\n将建表语句发送到新节点执行\n\n```\n执行本地表的ddl\n./sendfile.sh ckip.txt localtable.txt distable.txt\n./runcmd.sh ckip.txt \"/data/clickhouse/bin/clickhouse-client  --password xxx -mn < /home/jinyaqia/ck_tool/localtable.txt\"\n\n执行分布式表的ddl\n./runcmd.sh ckip.txt \"/data/clickhouse/bin/clickhouse-client  --password xxx -mn < /home/jinyaqia/ck_tool/distable.txt\"\n```\n\n其中\n\n```\n# sendfile.sh\n# 用于发送文件\n#!/usr/bin/env bash\n\nfile=''\narray=\"$@\"\ni=0\nfor el in $array\ndo\n    if [ ${i} -ne 0 ]\n\tthen\n\t\tfile=$file' '${el}\n    fi\n\tlet i++\ndone\n\nfor ip in $(cat $1|grep -v \"^#.*\\|^$\")\ndo\n    cmd=\"scp ${file} $ip:~/ck_tool/\"\n\t$cmd &\n\techo $ip,$cmd\ndone\n\nwait\necho 'done'\n```\n\n```\n# runcmd.sh\n# 用于执行命令\n#!/usr/bin/env bash\n\nfor ip in $(cat $1|grep -v \"^#.*\\|^$\")\ndo\n\techo $ip\n    ssh $ip \"${2}\" &\n    echo \"end ${ip}\"\ndone\n\nwait\n```\n\n## 2. 配置查询代理，从新节点查询数据\n## 3. 滚动迁移\n### 3.1 先停掉待迁移节点的写入\n域名dns管理去掉待迁移节点，这里需要写入端没有直接缓存ck节点ip，确保在数据复制的过程中，新的数据不会写入待迁移的节点\n### 3.2 ck节点一对一复制数据\n采用clickhouse copier的方式进行迁移，生成所有本地表的迁移配置，调用迁移脚本进行数据复制\n\n1. 导出需要进行迁移的表，可以从system.tables中查询并导出\n\n```\necho \"select database||'.'||name,engine_full  from system.tables where database != 'system' and engine not in ('Distributed','View','MaterializedView')  order by name desc\" | /data/clickhouse/bin/clickhouse-client --password xxx --port 9000 > dbtable.txt.all\n\n```\n\n导出的文件去掉.inner.的表，物化视图等普通本地表导完后再处理\n如果有转义的,如'\\，手工替换掉就可以了\n\n2. 将脚本和文件都上传到接口机，由接口机发送到新节点,脚本内容后文给出\n\n```\ndbtable.txt.all  需要迁移的表\ndbtable.txt.todo 需要执行迁移的表\ntable_copy_copier.py  迁移脚本\ntable_check.py  数据一致性检查脚本\nget_error.py  处理迁移异常的表\nipmap.txt     旧节点复制到新节点的ip对应关系\ncopier_template.xml 复制工具配置模板\nzk.xml  复制工具zk配置\n\n发送命令\n./sendfile.sh ckip.txt dbtable.txt.all dbtable.txt.todo table_copy_copier.py table_check.py get_error.py copier_template.xml zk.xml\n```\n\n3. 执行复制\n\n```\nnohup python3 table_copy_copier.py 2>&1 >copy.log &\n```\n\n4. 校对、检查和重跑\n\n```\n批量校对\npython3 table_check.py\npython3 get_error.py\n\n有异常的表会写入dbtable.txt.1\n其中新节点数据条数少的写入error.txt.less\n新节点数据条数多了的写入error.txt.more\n\n手动核对下dbtable.txt.1里的表迁移异常的原因，可以查clickhouse-copier的日志，如果要重跑，则生成新的dbtable.txt.todo, 删掉zk中对应表的节点，然后调用table_copy_copier.py重新复制\n\n```\n### 3.3 更新ck集群配置remote_servers，增加新节点，去掉旧节点，等待ck server自动加载更新配置，此时查询会落到新节点上\n\n### 3.4 配置域名，使数据写入新节点\n\n## 4. 退回机器\n\n# 注意点\n\n1. 该方案在数据复制的时候要求旧节点和新节点都不会写数据，这个需要写入的时候不写分布式表\n2. 复制任务重跑时，需要删掉zk节点的数据\n3. 由于复制任务会在zk上创建比较多的节点数，为了降低zk的延迟，不要用ck集群的zk，可以单独搭建一个zk集群\n4. table_copy_copier.py调用 clickhouse copier命令后，即使主进程关闭了，copier任务也会一直运行\n5. table_copy_copier.py配置了60个子线程用于调用copier任务，如果要降低带宽的使用，可以调低并行度\n\n\n# 附录：\n## table_copy_copier.py\n\n```python\nimport sys,json;\nimport socket;\nimport os;\nfrom concurrent.futures import ThreadPoolExecutor,as_completed\nimport queue,threading;\nimport time,functools\n\ndef get_time():\n    time_now=int(time.time())\n    time_local=time.localtime(time_now)\n    return time.strftime(\"%Y-%m-%d %H:%M:%S\",time_local)\n   \ndef exec_command(fo,db,table,command_template,ip,task_file):\n    try:\n        task_path='/copytask/{}/{}/{}'.format(ip,db,table)\n        base_dir=table\n        if not os.path.exists(base_dir):\n            os.makedirs(base_dir)\n        command=command_template.format(task_path=task_path,base_dir=base_dir,task_file=task_file)\n        print(\"start thread num={} command={} pid={}\".format(threading.get_ident(),command,os.getpid()))\n        f = os.popen(command)\n        print(\"done thread num={} command={} pid={}\".format(threading.get_ident(),command,os.getpid()))\n        res=\"start {}.{} time={}\\n\".format(db,table,get_time())\n        fo.write(res)\n        fo.flush()\n    except:\n        error=\"start {}.{} error\\n\".format(db,table)\n        fo.write(error)\n        fo.flush()\n    return f\n\ndef read_db_table(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read().split('\\n')\n    finally:\n        f.close()\n    return lines\n\ndef read_file(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read()\n    finally:\n        f.close()\n    return lines\n\ndef write_file(file_name,content):\n    try:\n        f = open(file_name,\"w+\")\n        f.write(content)\n    finally:\n        f.close()\n\ndef get_local_ip():\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect(('8.8.8.8', 80))\n        localIp = s.getsockname()[0]\n    finally:\n        s.close()\n    return localIp\n\ndef task_done_callback(future,dbtable_str):\n    try:\n        data = future.result()\n        res=\"done {} time {}\\n\".format(dbtable_str,get_time())\n    except CancelledError:\n        res=\"error {} time {}\\n\".format(dbtable_str,get_time())\n    finally:\n        fo = open(\"result.txt\",\"a+\")\n        fo.write(res)\n        fo.flush()\n        print('result',data.read())\n        fo.close()\n\n\nif __name__ == '__main__':\n\n    ipmap=json.loads(read_file(\"ipmap.txt\"))\n    print(ipmap)\n    localIp = get_local_ip()\n    print(localIp)\n    remoteIp = ipmap[localIp]\n    print(remoteIp)\n    # copier template\n    copier_template = read_file(\"copier_template.xml\")\n    print(copier_template)\n    # db.table needs to copy\n    lines = filter(None,read_db_table(\"dbtable.txt.todo\"))\n    print(lines)\n    command_template=\"/data/clickhouse/bin/clickhouse copier --config zk.xml --task-path {task_path} --task-file {task_file} --base-dir {base_dir}\"\n    result_file = open(\"task_log.txt\",\"a+\")\n    pool = ThreadPoolExecutor(60)\n    waitQueue = queue.Queue()\n    for line in lines:\n        waitQueue.put(line)\n    \n    while(waitQueue.qsize()>0):\n        line_arr = waitQueue.get().split('\\t')\n        com=line_arr[0]\n        task_file=\"task_{}.xml\".format(com)\n        dbtable=com.split(\".\")\n        content=copier_template.format(source_host=remoteIp,target_host=localIp,database=dbtable[0],table=dbtable[1],engine=line_arr[1])\n        write_file(task_file,content)\n        task = pool.submit(exec_command, result_file, dbtable[0],dbtable[1], command_template, remoteIp, task_file).add_done_callback(functools.partial(task_done_callback,dbtable_str=com))\n        \n```\n\n## table_check.py\n\n```\nimport sys,json;\nimport socket;\nimport os;\nfrom concurrent.futures import ThreadPoolExecutor,as_completed,CancelledError\nimport queue,threading;\nimport time,functools\n\ndef get_time():\n    time_now=int(time.time())\n    time_local=time.localtime(time_now)\n    return time.strftime(\"%Y-%m-%d %H:%M:%S\",time_local)\n   \ndef exec_command(fo, dbtable_str,sql):\n    \n    try:        \n        dbtable=dbtable_str.split(\".\")\n        command=sql.format(db=dbtable[0],table=dbtable[1])\n        print(\"start thread num={} command={} pid={}\".format(threading.get_ident(),command,os.getpid()))\n        f = os.popen(command)\n        res=\"start {} time={}\\n\".format(dbtable_str,get_time())\n        fo.write(res)\n        fo.flush()\n    except:\n        error=\"start {} error\\n\".format(dbtable_str)\n        fo.write(error)\n        fo.flush()\n    return f\n\ndef read_db_table(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read().split('\\n')\n    finally:\n        f.close()\n    return lines\n\ndef read_file(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read()\n    finally:\n        f.close()\n    return lines\n\ndef get_local_ip():\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect(('8.8.8.8', 80))\n        localIp = s.getsockname()[0]\n    finally:\n        s.close()\n    return localIp\n\ndef task_done_callback(future,fo, dbtable_str,engine):\n    res=\"\"\n    try:\n        data = future.result()\n        counts = data.read().split('\\n')\n        line1= counts[0].split('\\t')\n        line2=counts[1].split('\\t')\n        count1=count2=0\n        if(line1[0]=='a'):\n            count1=line1[1]\n            count2=line2[1]\n        else:\n            count1=line2[1]\n            count2=line1[1]\n        res=\"done|{}\\t{}|result@{}@{}@{}\\n\".format(dbtable_str,engine,count1,count2,int(count1)-int(count2))\n    except CancelledError:\n        res=\"error {} \\n\".format(dbtable_str)\n    finally:\n        fo.write(res)\n        fo.flush()\n        print('result',res)\n    \n\n\nif __name__ == '__main__':\n    ipmap=json.loads(read_file(\"ipmap.txt\"))\n    print(ipmap)\n    localIp = get_local_ip()\n    print(localIp)\n    remoteIp = ipmap[localIp]\n    print(remoteIp)\n    sql=\"echo \\\"select 'a',count() from remote('\"+remoteIp+\"','{db}','{table}','default','IARYxRcr') union all select 'b',count() from {db}.{table};\\\" |/data/clickhouse/bin/clickhouse-client  --password IARYxRcr\"\n    lines = filter(None,read_db_table(\"dbtable.txt.all\"))\n    print(lines)\n    fo = open(\"checkresult.txt\",\"w+\")\n    fo1 = open(\"checkresult1.txt\",\"w+\")\n    pool = ThreadPoolExecutor(60)\n    waitQueue = queue.Queue()\n    for line in lines:\n        waitQueue.put(line)\n    \n    while(waitQueue.qsize()>0):\n        line_arr = waitQueue.get().split('\\t')\n        com=line_arr[0]\n        engine=line_arr[1]\n        task = pool.submit(exec_command, fo, com, sql).add_done_callback(functools.partial(task_done_callback,dbtable_str=com,fo=fo1,engine=engine))\n        \n```\n\n## get_error.py\n\n```\nimport sys\nimport json\nimport socket\nimport os\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport queue\nimport threading\nimport time\nimport functools\n\n\ndef read_file(file_name):\n    try:\n        f = open(file_name)\n        lines = f.read().split('\\n')\n    finally:\n        f.close()\n    return lines\n\n\nif __name__ == '__main__':\n    fo = open(\"dbtable.txt.1\", \"w+\")\n    done = open('finish.txt', \"w+\")\n    lines = read_file(\"checkresult1.txt\")\n    # 比旧节点少了\n    err_less = open('error.txt.less', 'w+')\n    # 比旧节点多了\n    err_more = open('error.txt.more', 'w+')\n    todo = 0\n    to_delete = 0\n    finish = 0\n    for line in lines:\n        if(line != ''):\n            line_arr = line.split('|')\n            print(line_arr)\n            result = line_arr[2].split('@')\n            dbtable = line_arr[1].split('\\t')[0]\n            old=int(result[1])\n            new = int(result[2])\n            diff=int(result[3])\n    \n            if(diff != 0 and new == 0):\n                todo += 1\n                fo.write(line_arr[1]+'\\n')\n            elif(diff != 0 and new != 0):\n                todo += 1\n                to_delete += 1\n                fo.write(line_arr[1]+'\\n')\n            else:\n                finish += 1\n                done.write(dbtable+'\\n')\n\n            if(diff>0):\n                err_less.write(\"{} {}\\n\".format(dbtable,result))\n            elif(diff<0):\n                err_more.write(\"{} {}\\n\".format(dbtable,result))\n\n    print(\"finish={}, todo={}\".format(finish, to_delete, todo))\n    fo.close()\n    done.close()\n    err_less.close()\n    err_more.close()\n\n```\n\n## copier_template.xml\n\n```\n<yandex>\n    <remote_servers>\n        <source_cluster>\n            <shard>\n                <internal_replication>false</internal_replication>\n                    <replica>\n                        <host>{source_host}</host>\n                        <port>9000</port>\n                        <user>default</user>\n                        <password>IARYxRcr</password>\n                        <secure>0</secure>\n                    </replica>\n            </shard>\n        </source_cluster>\n\n        <destination_cluster>\n            <shard>\n                <internal_replication>false</internal_replication>\n                    <replica>\n                        <host>{target_host}</host>\n                        <port>9000</port>\n                        <user>default</user>\n                        <password>IARYxRcr</password>\n                        <secure>0</secure>\n                    </replica>\n            </shard>89\n        </destination_cluster>\n    </remote_servers>\n\n    <max_workers>32</max_workers>\n\n    <!-- Setting used to fetch (pull) data from source cluster tables -->\n    <settings_pull>\n        <readonly>1</readonly>\n    </settings_pull>\n\n    <settings_push>\n        <readonly>0</readonly>\n    </settings_push>\n\n     <settings>\n        <connect_timeout>3</connect_timeout>\n         <insert_distributed_sync>1</insert_distributed_sync>\n    </settings>\n\n    <tables>\n        <table_hits>\n            <!-- Source cluster name (from <remote_servers/> section) and tables in it that should be copied -->\n            <cluster_pull>source_cluster</cluster_pull>\n            <database_pull>{database}</database_pull>\n            <table_pull>{table}</table_pull>\n\n            <!-- Destination cluster name and tables in which the data should be inserted -->\n            <cluster_push>destination_cluster</cluster_push>\n            <database_push>{database}</database_push>\n            <table_push>{table}</table_push>\n\n            <engine>\n            ENGINE={engine}\n            </engine>\n            <sharding_key>rand()</sharding_key>\n\n        </table_hits>\n    </tables>\n</yandex>\n```\n\n## ipmap.txt\n\n```\n{\n    \"10.69.28.1\": \"10.69.29.11\",\n    \"10.69.29.2\": \"10.69.28.22\",\n    \"10.69.29.3\": \"10.69.29.33\",\n    \"10.69.29.4\": \"10.69.29.44\"\n}\n```\n\n## zk.txt\n```\n<yandex>\n    <logger>\n        <level>trace</level>\n        <size>100M</size>\n        <count>3</count>\n    </logger>\n    <zookeeper>\n        <node index=\"0\">\n            <host>zkhost</host>\n            <port>2181</port>\n        </node>\n    </zookeeper>\n</yandex>\n```","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"union all并行查询提速","url":"/clickhouse/optimize-unionall/","content":"# union all并行查询提速\n\n<!--more-->\n# 原查询是：\n\n```\nSELECT\n    a.count AS reqcount,\n    b.count AS tenseccount,\n    c.count AS thirtyseccount,\n    d.count AS onemincount,\n    e.count AS fivemincount,\n    f.count AS failcount,\n    g.count AS verifycount,\n    h.count AS verifysuccesscount,\n    i.count AS cost,\n    j.count AS onetofivemincount\nFROM\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS a\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime < 10000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS b\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 10000) AND (g.receiptusetime < 30000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS c\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 30000) AND (g.receiptusetime < 60000) AND (g.receiptusetime >= 60000) AND (g.receiptusetime < 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS d\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS e\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND ((g.status = '0') OR (g.status = '2')) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS f\n,\n(\n    SELECT count(0) AS count\n    FROM dis_sms_captcha AS c\n    WHERE (1 = 1) AND (c.appid = '5001') AND (c.its >= 1587312000) AND (c.its <= 1587484800)\n) AS g\n,\n(\n    SELECT count(0) AS count\n    FROM dis_sms_captcha AS c\n    WHERE (1 = 1) AND (c.appid = '5001') AND (c.mobileverifycount > '0') AND (c.its >= 1587312000) AND (c.its <= 1587484800)\n) AS h\n,\n(\n    SELECT sum(toDecimal128(notEmpty(price), 4)) AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.appid = '5001') AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS i\n,\n(\n    SELECT count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 60000) AND (g.receiptusetime < 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) AS j\nWHERE 1 = 1\n```\n\n# 查询是串行的，执行结果为:\n\n```\n1 rows in set. Elapsed: 6.443 sec. Processed 8.20 million rows, 47.30 MB (1.27 million rows/s., 7.34 MB/s.)\n```\n\n# 使用union all是查询可以并行，sql变为\n```\nSELECT\n   name,count\nFROM (\n(\n    SELECT 'a' as name,count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'b',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime < 10000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'c',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 10000) AND (g.receiptusetime < 30000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) \nunion all\n(\n    SELECT 'd',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 30000) AND (g.receiptusetime < 60000) AND (g.receiptusetime >= 60000) AND (g.receiptusetime < 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n) \nunion all\n(\n    SELECT 'e',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'f',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND ((g.status = '0') OR (g.status = '2')) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'g',count(0) AS count\n    FROM dis_sms_captcha AS c\n    WHERE (1 = 1) AND (c.appid = '5001') AND (c.its >= 1587312000) AND (c.its <= 1587484800)\n) \nunion all\n(\n    SELECT 'h',count(0) AS count\n    FROM dis_sms_captcha AS c\n    WHERE (1 = 1) AND (c.appid = '5001') AND (c.mobileverifycount > '0') AND (c.its >= 1587312000) AND (c.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'i',sum(toDecimal128(notEmpty(price), 4)) AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.appid = '5001') AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\nunion all\n(\n    SELECT 'j',count() AS count\n    FROM dis_sms_gateway AS g\n    WHERE (1 = 1) AND (g.receiptusetime >= 60000) AND (g.receiptusetime < 300000) AND (g.its >= 1587312000) AND (g.its <= 1587484800)\n)\n)\n\n```\n\n# 查询结果为：\n```\n10 rows in set. Elapsed: 0.376 sec. Processed 8.20 million rows, 47.30 MB (21.83 million rows/s., 125.94 MB/s.)\n```","tags":["sql优化"],"categories":["clickhouse"]},{"title":"clickhouse+JuiceFS冷热数据分层+读写分离的方案","url":"/clickhouse/juicefs/","content":"\n# clickhouse+JuiceFS冷热数据分层+读写分离的方案\n\n## JuiceFS简介\n\njuiceFS的介绍可以参考[juiceFS github](https://github.com/juicedata/juicefs)\n\nJuiceFS 是在 GNU Affero General Public License v3.0 下发布的高性能 POSIX 文件系统。它专门针对云原生环境进行了优化。使用 JuiceFS 存储数据，数据本身会持久化到对象存储（如 S3、oss）中，数据对应的元数据可以根据需要持久化到 Redis、MySQL、SQLite 等各种数据库引擎中。\n\nJuiceFS 可以简单方便地将海量云存储直接连接到大数据、机器学习、人工智能以及已经投入生产环境的各种应用平台，无需修改代码，您可以像使用本地存储一样高效地使用海量云存储。\n\n<!--more-->\n\n## 为啥会考虑使用JuiceFS\n\n与Hadoop 生态组件通常依赖 HDFS 作为底层的数据存储不同，ClickHouse 使用本地盘来自己管理数据，[社区建议](https://clickhouse.tech/docs/en/operations/tips/#storage-subsystem)使用 SSD +raid盘的方式作为存储介质来提升性能。但受限于本地盘的容量上限以及 SSD 盘的价格，用户很难在容量、成本和性能这三者之间找到一个好的平衡。由于JuiceFS 是基于对象存储实现并完全兼容 POSIX 的开源分布式文件系统，同时 JuiceFS 的数据缓存特性可以智能管理查询热点数据，非常适合作为 ClickHouse 的存储系统，下面将详细介绍这个方案。\n\n## 方案细节\n\n由于JuiceFS完全兼容POSIX协议，因此可以将JuiceFS mount到本地磁盘，不同的节点可以都使用同一个JuiceFS进行mount，并使用不同的目录进行区别。\n\n为了演示，使用hdfs作为JuiceFS的存储，使用redis作为JuiceFS的元数据存储。\n\n### 初始化JuiceFS\n\n```shell\njuicefs  format --storage hdfs \\\n--bucket nn1.local.hadoop3.hy:8020,nn2.local.hadoop3.hy:8020 \\\n--access-key <user>@<token> \\\nredis://:<pwd>@<ip>:<port>/<db>  jfsck\n```\n\n### mount JuiceFS\n\n```shell\nsudo juicefs mount --cache-dir /data5/jfsCache -d redis://:<pwd>@<ip>:<port>/<db> /mnt/jfshdfs\n```\n\n### 配置clickhouse磁盘和存储策略\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<clickhouse>\n    <storage_configuration>\n        <disks>            \n            <default>\n             <keep_free_space_bytes>1048576000</keep_free_space_bytes>\n            </default>\n            <data2>\n                <path>/data2/clickhouse/data/</path>\n                <keep_free_space_bytes>1048576000</keep_free_space_bytes>\n            </data2>   \n            <hdfs>\n                <path>/mnt/jfshdfs/68/</path>\n            </hdfs>\n        </disks>\n        <policies>\n            <default>\n                <volumes>\n                    <cold>\n                        <disk>default</disk>\n                        <disk>data2</disk>\n                    </cold>            \n                </volumes>\n                <move_factor>0.2</move_factor>\n            </default>\n            <cold>\n                <volumes>\n                    <backup>\n                        <disk>hdfs</disk>\n                    </backup>\n                </volumes>\n            </cold>\n            <hot_cold>\n                <volumes>\n                    <hot>\n                        <disk>default</disk>\n                        <disk>data2</disk>\n                        <max_data_part_size_bytes>104857600</max_data_part_size_bytes>\n                    </hot>\n                    <cold>\n                        <disk>hdfs</disk>\n                    </cold>                                    \n                </volumes>\n                <move_factor>0.2</move_factor>\n            </hot_cold>\n          </policies>\n    </storage_configuration>\n</clickhouse>\n```\n\n配置中定义了`hot_cold`策略，该策略中包含`hot`和`cold`两个磁盘卷，`hot`中指定的是两个本地磁盘，`cold`中指定的是JuiceFS远程共享文件系统，`max_data_part_size_bytes`表示可以存储在卷的任何磁盘上的部分的最大大小。如果合并部分的大小估计大于 max_data_part_size_bytes，则该部分将被写入下一个卷。基本上，此功能允许将新/小部件保留在热 (SSD) 卷上，并在它们达到大尺寸时将它们移至冷 (HDD) 卷。为了测试这里设置为100M`move_factor`配置表示当磁盘的容量超过 80% 时触发数据移动到 JuiceFS。\n\n重启clickhouse-server，查看存储策略是否生效\n\n![image-20220103164803141](../../images/clickhouse/juicefs-4.png)\n\n### 建表测试\n\n#### 建表\n\n新建一张按天分区的表，默认的存储策略设置为`hot_cold`, 设置超过20天的数据`move`到`cold`磁盘卷中\n\n```\nCREATE TABLE default.t_report_yspwl_dm_transm_mk_info2_day\n(\n    `day` Date DEFAULT toDate(its),\n    `its` UInt32,\n    `appid` Int32,\n    `line` Int32,\n    `user_isp` LowCardinality(String),\n    `user_province` LowCardinality(String),\n    `cdnip` LowCardinality(String),\n    `networktype` LowCardinality(String),\n    `game_name` LowCardinality(String),\n    `p2p` Int32,\n    `p2p_origin_type` LowCardinality(String),\n    `harddecode` Int32,\n    `bad_cnt` Int64,\n    `log_cnt` Int64,\n    `tot_log_cnt` Int64,\n    `repeat_times` Int64,\n    `_cnt` UInt32 DEFAULT CAST(1, 'UInt32'),\n    `peak_flag` Int32\n)\nENGINE = MergeTree\nPARTITION BY toYYYYMMDD(day)\nORDER BY its\nTTL day + toIntervalDay(20) TO VOLUME 'cold'\nSETTINGS index_granularity = 8192, storage_policy = 'hot_cold'\n```\n\n#### 插入数据\n\n1. 当插入20天前的数据，parts会直接写入到`cold`中\n2. 当插入`2021-12-17`号的数据，查看parts分布\n\n![image-20220103153506496](../../images/clickhouse/juicefs-1.png)\n\n过一会等后台线程merge后再查看,可以看到17号的数据大于100MB的parts被存进了cold存储中\n\n![image-20220103154047920](../../images/clickhouse/juicefs-2.png)\n\n查看目录文件\n\n![image-20220103154700397](../../images/clickhouse/juicefs-3.png)\n\n## 一种读写分离的思路\n\n采用JuiceFS存储clickhouse冷数据，可以有效的扩展clickhouse单节点磁盘不足的问题，同时由于JuiceFS的共享特性，可以引申出一种读写分离的架构，如下图所示。对于ck本地表`default.table1`，在离线场景下，可以从k8s上启动临时的ck server，将离线数据写入到对应表中，当分区的数据写完后，可以detach partition，然后将这个partition对应的目录move到正式集群对应的节点对应的目录中去，再在正式集群中attach进行使用。此做法在重跑大量历史数据时，可以大大减轻正式集群的节点写入压力。\n\n<img src=\"../../images/clickhouse/juicefs-5.png\" alt=\"企业微信截图_7f1369c4-b3e7-46c2-9bbd-87219e5dd8fe\" style=\"zoom:30%;\" />\n\n## 参考\n\n[[ClickHouse 存算分离架构探索](https://juicefs.com/blog/cn/posts/clickhouse-disaggregated-storage-and-compute-practice/)](https://juicefs.com/blog/cn/posts/clickhouse-disaggregated-storage-and-compute-practice/)\n\n[Shopee ClickHouse 冷热数据分离存储架构与实践](https://mp.weixin.qq.com/s/eELcw0v2U9UXaHnfADj3tQ)\n\n","tags":["clickhouse"],"categories":["clickhouse"]},{"title":"about","url":"/about/index.html"},{"title":"Plugins","url":"/plugins/index.html","content":"\n### Hexo Plugins\n\n* [hexo-generator-searchdb](https://github.com/next-theme/hexo-generator-searchdb)\n* [hexo-filter-emoji](https://github.com/next-theme/hexo-filter-emoji)\n* [hexo-pangu](https://github.com/next-theme/hexo-pangu)\n* [hexo-filter-mathjax](https://github.com/next-theme/hexo-filter-mathjax)\n* [hexo-word-counter](https://github.com/next-theme/hexo-word-counter)\n\n### NexT Plugins\n\n* [hexo-next-three](https://github.com/next-theme/hexo-next-three)\n* [hexo-next-fireworks](https://github.com/next-theme/hexo-next-fireworks)\n* [hexo-next-exif](https://github.com/next-theme/hexo-next-exif)\n\n---\n\n* Visit the [Awesome NexT](https://github.com/next-theme/awesome-next) list for more plugins.\n"},{"title":"分类","url":"/categories/index.html"},{"title":"标签","url":"/tags/index.html"},{"title":"404","url":"//404.html","content":"\n```\n  ██╗  ██╗ ██████╗ ██╗  ██╗    ███╗   ██╗ ██████╗ ████████╗\n  ██║  ██║██╔═████╗██║  ██║    ████╗  ██║██╔═══██╗╚══██╔══╝\n  ███████║██║██╔██║███████║    ██╔██╗ ██║██║   ██║   ██║\n  ╚════██║████╔╝██║╚════██║    ██║╚██╗██║██║   ██║   ██║\n       ██║╚██████╔╝     ██║    ██║ ╚████║╚██████╔╝   ██║\n       ╚═╝ ╚═════╝      ╚═╝    ╚═╝  ╚═══╝ ╚═════╝    ╚═╝\n\n      ███████╗ ██████╗ ██╗   ██╗███╗   ██╗██████╗\n      ██╔════╝██╔═══██╗██║   ██║████╗  ██║██╔══██╗\n      █████╗  ██║   ██║██║   ██║██╔██╗ ██║██║  ██║\n      ██╔══╝  ██║   ██║██║   ██║██║╚██╗██║██║  ██║\n      ██║     ╚██████╔╝╚██████╔╝██║ ╚████║██████╔╝\n      ╚═╝      ╚═════╝  ╚═════╝ ╚═╝  ╚═══╝╚═════╝\n```\n"}]